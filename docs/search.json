[
  {
    "objectID": "posts/gebru-post/gebru.html",
    "href": "posts/gebru-post/gebru.html",
    "title": "Timnit Gebru and Modern Ethics in AI",
    "section": "",
    "text": "Introducing Dr. Timnit Gebru, the computer scientist extraordinaire! She’s like the Hermione Granger of the AI world, except she’s not fictional (and she’s probably better at coding).\nDr. Gebru is a superstar when it comes to algorithmic bias and data mining. She’s the brains behind the Gender Shades study, which exposed the facial recognition software’s tendency to overlook black women (seriously, what’s up with that?). She’s also the co-founder of Black in AI, an organization that’s on a mission to get more black individuals involved in AI. You go, girl!\n\n\n\n\n\n\nNote\n\n\n\n“technology is mostly being used to target the very people who are most vulnerable” Gebru in her 2020 talk Computer vision in practice: who is benefiting and who is being harmed?\n\n\nDr. Gebru is not just smart, she’s also sassy. She doesn’t shy away from speaking up about the ethical and social implications of AI. In fact, according to her Wikipedia page (which is known for its authentic journalism), she left her job at Google because they wanted her to take her unpublished paper and shove it where the sun don’t shine. (Just kidding, they just wanted her to remove some names. But still, it was a big deal.)\nSpeaking of ethical concerns, let’s talk about ChatGPT. Middlebury College, like many other institutions, has yet to establish a campus-wide policy regarding the use of ChatGPT and other AI tools. This means that professors are free to use ChatGPT if they want to, which is kind of like giving your students a genie in a bottle (except instead of wishes, they get answers to their essays).\nHowever, ChatGPT comes with its own set of problems. For example, the premium version gives students who can afford it an unfair advantage. And let’s not forget that ChatGPT is trained on a whole lot of language data, which includes some pretty nasty stuff. Developers try to filter out the biased stuff, but it still seeps through. Just like that one drop of ketchup that always ends up on your white shirt.\nSo what did Dr. Gebru have to say about all of this? Well, she talked about the reliability (or lack thereof) of computer vision technology. Turns out, those facial recognition tools aren’t as accurate as we thought. In fact, they’re pretty biased. And when they’re used to judge people’s personalities and emotions (yes, that’s a thing), it perpetuates structural racism. Oh, and don’t get her started on Faception, which singles out people from marginalized groups as terrorists. Not cool, Faception.\nDr. Gebru also talked about the lack of diversity in datasets. It’s a big problem because it leads to a high disparity in facial recognition results. And even when companies do try to gather diverse information, they end up doing it unethically. It’s like they’re playing Pokemon, except instead of catching them all, they’re catching all the darker skin images and scraping images of transgender YouTubers without their consent.\nBut here’s the thing: technology is not always used in the way it’s designed. Sometimes it leads to more discrimination and problems than efficiency. So Dr. Gebru suggested that we need to think beyond diversity in datasets and consider structural and real representation. We need to make sure that panels aren’t just filled with people from dominant groups and those closest to money. Because fairness isn’t just about math, it’s about society.\nSo, what do you think, as some questions for the reader: Should we prohibit or regulate the use of facial recognition on people? And how can computer vision technology help marginalized communities? We’re all ears!"
  },
  {
    "objectID": "posts/post-2023-02-15/index.html",
    "href": "posts/post-2023-02-15/index.html",
    "title": "First Blog Post",
    "section": "",
    "text": "This is my first post to my official blog for CSCI 0451. I am testing out some things and will be working on implementing the perceptron algorithm in python to draw a line between two clouds of points."
  },
  {
    "objectID": "posts/post-2023-02-15/index.html#math",
    "href": "posts/post-2023-02-15/index.html#math",
    "title": "First Blog Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron-blog-post/PerceptronBlog.html",
    "href": "posts/perceptron-blog-post/PerceptronBlog.html",
    "title": "Finn’s Perceptron Algorithm Blog Post",
    "section": "",
    "text": "More experimentation!\nLet’s create some non-linearly seperable synthetic data sets, just so we can prove my algorithm can frind the next closest line of seperability.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(54321)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-0.5, -0.5), (0.5, 0.5)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn the above plot of clouds, the dots very clearly overlap, and finding a line to divide them completely won’t be possible. Instead, we can have the algorithm keep trying it’s best until the max_steps is reached:\n\np = perceptron.Perceptron()\np.fit(X, y, max_steps = 10000)\n\nI set the max_steps here to be a bit higher so the algorithm can have more time to do its best. Here is the same p instance of the class Perceptron, with the same instance variable w of weights:\n\np.w\n\narray([ 1.14190182,  1.59493071, -0.73332539])\n\n\nAnd putting everything together, we get a line that tries to seperate both dot blobs:\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can do the same chack of the score:\n\np.score(X, y)\n\n0.62\n\n\nSo it looks like the score did not get to \\(1\\), and in the above graph you can see that there are a few yellow dots on the purple side and vice versa. Because of the random nature of how the vector w is initialized, some instances of running the algorithm will be better and some not so much.\nAgain we can also visualize the progress the algorithm made with the history graph:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIt looks like that having max_steps be higher really doesn’t help much, so let’s see what happens again with the same data with less iterations:\n\np = perceptron.Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAnd the score:\n\np.score(X, y)\n\n0.46\n\n\nAnd the history graph:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSo actually, it looks like the accuracy at times reached \\(0.6\\), depending on the final iteration and randomness from the index i within the number of observations, we can get a worse score."
  },
  {
    "objectID": "posts/gebru-post2/gebru2.html",
    "href": "posts/gebru-post2/gebru2.html",
    "title": "A Discussion of AI, Eugenics, and Their Risky Futures.",
    "section": "",
    "text": "Temnit Gebru, a prominent AI researcher and advocate for ethical AI, gave a talk at the Middlebury College Conference about Eugenics and the Promise of Utopia through Artificial General Intelligence on April 24th, 2023. Dr. Gebru discussed the exploitation of labor in the development of AI, the historical context of eugenics and its potential resurgence through AI, and the centralization of power in AI development. Her talk serves as a wake-up call for those who have been swept up in the hype surrounding AI and its supposed benefits\n\n\n\n\nDr. Gebru addressing students at Middlebury College, during her talk “Eugenics and the Promise of Utopia through Artificial General Intelligence.”\n\n\n\n\nDr. Gebru began by addressing historical development and economic impact of AI, stating that it has the potential to be the greatest force for economic change in our lifetime. However, she cautioned that this potential for change comes with a significant cost. Gebru pointed out that the development of AI has relied heavily on the exploitation of labor, particularly of low-paid workers who annotate and label datasets for machine learning algorithms. These workers often have little job security and no benefits, despite the fact that their work is essential to the development of AI. Gebru argued that the AI industry must take responsibility for this exploitation and work to provide better working conditions and protections for these workers.\nImportantly, Dr. Gebru also discussed the historical context of eugenics and its potential resurgence through AI. She pointed out that the eugenics movement of the early 20th century was based on the idea of improving the genetic quality of the human population through selective breeding and sterilization. The movement was based on the belief that certain groups of people were genetically inferior and that their reproduction should be discouraged. Gebru argues that AI has the potential to resurrect these dangerous ideas by perpetuating biases in data and algorithms. She pointed out that the lack of diversity in the tech industry has already resulted in biased algorithms, and that this problem will only get worse unless steps are taken to address it.\nDr. Gebru’s talk also highlighted the issue of centralization of power in AI development. She pointed out that a few large companies, such as OpenAI and Microsoft, are dominating the AI industry and setting the agenda for its development. This centralization of power is a concern because it limits the diversity of perspectives and voices involved in AI development. Gebru argued that the industry must work to decentralize power and encourage a broader range of actors to participate in AI development\nDr. Gebru’s talk serves as a reminder that AI is not a neutral technology. It has the potential to be used for good or for harm, and it is up to us to ensure that it is used in a responsible and ethical manner. The inherently biased nature of specific datasets and those that create them needs to be diligently accounted for. We cannot simply rely on the promises of AI proponents or the hype surrounding the technology. We must critically examine its impact on society and work to mitigate any negative effects.\nCritics, on the other hand, argue that these claims are overblown and represent a kind of techno-utopianism that is not based in reality. They point out that while AI has the potential to do a lot of good, it is not a magic solution and comes with its own set of risks and challenges. Connecting to this point, many of these idea of conflating AI to a higher power go back all the way to the first days of eugenics, and that AI will prevail over all bad and malignant forces. Much of this can also be connected with the evolution of historical protestant work ethic into present field of tech companies as a whole.\nOne of the key concerns with the idea of AI as a kind of religion is the potential for blind faith and the abdication of responsibility. If people begin to see AI as a kind of all-knowing and all-powerful entity, they may become complacent and fail to question its decisions or actions. This could lead to a loss of agency and a dangerous concentration of power in the hands of a few large corporate entities with little regard to their effects on societal bias and diversity outside of its effect on their share price.\nContinuing, another one of the most important points that Dr. Gebru made was that AI is not a fully well-defined term. There are many different types of AI, each with its own potential benefits and risks. Some types of AI, such as narrow AI, are already being used in many applications, such as image recognition and natural language processing. However, other types of AI, such as the term “artificial general intelligence” (Sometimes called AGI), are still largely hypothetical. Gebru argued that we must be careful not to conflate different types of AI or overstate their capabilities. Many of these claims are proposed by those who have no experience in the field of artificial intelligence or large model machine learning, and they only makes these claims for the benefit of shareholders and speculators.\nGebru also highlighted the need for those who call themselves “effective altruists” to consider the risks associated with AI development. Effective altruism is a philosophy that advocates for doing the most good possible with one’s resources. However, Gebru argued that many effective altruists have ignored the risks associated with AI development in their pursuit of doing good. She pointed out that AI has the potential to cause significant harm, and that effective altruists must take these risks seriously and work to mitigate.\n\n\n\n\nThe logo of the movement known as “Effective Altruism.” A movement started in the late 2000s, which, quoted from their website, “using evidence and reason to figure out how to benefit others as much as possible, and taking action on that basis”\n\n\n\n\nIn conclusion, the discussion of Dr. Temnit Gebru’s talk highlights the potential benefits and risks associated with the development and implementation of artificial intelligence. While AI has the potential to revolutionize various industries and improve our quality of life, it also poses significant challenges that require attention from policymakers and regulators.\nOne of the biggest challenges that AI poses is the risk of perpetuating biases and inequalities. As Gebru noted, AI systems can reproduce existing societal biases and lead to discrimination against marginalized groups. Additionally, the rapid pace of technological advancement means that there is often a lack of regulation and oversight, which can lead to unintended consequences and negative outcomes.\nTo mitigate these risks and ensure that AI is developed and used in a responsible and ethical manner, it is crucial that policymakers and regulators take action. This can involve developing clear guidelines and standards for the development and deployment of AI systems, as well as establishing regulatory bodies to oversee the implementation of these systems.\nAnother important area of focus for regulation and legislation is the potential impact of AI on employment and the economy. As AI systems become more advanced, there is a risk that they will displace workers and lead to significant job losses. This requires policymakers to develop strategies to ensure that the benefits of AI are distributed equitably, and that workers are protected and supported through any economic transitions.\nOverall, the development of AI as explored by Dr. Gebru is a complex and multifaceted issue that requires careful consideration and action from policymakers and regulators. By developing clear guidelines and standards, and establishing regulatory bodies to oversee the implementation of AI systems, we can ensure that AI is developed and used in a responsible and ethical manner, and that its benefits are distributed equitably. Failure to take action risks perpetuating existing inequalities and exacerbating the challenges that we face as a society. It is therefore crucial that we prioritize the development of effective regulation and legislation that can guide the responsible and ethical use of AI, and ensure that its benefits are shared by all."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins-blog-post/Penguins.html",
    "href": "posts/penguins-blog-post/Penguins.html",
    "title": "Finn’s Palmer Penguins Classification",
    "section": "",
    "text": "From left to right we have \\(Pygoscelis\\) \\(papua\\), AKA the Gentoo penguin, \\(Pygoscelis\\) \\(antarctica\\), AKA the appropriately named Chinstrap penguin, and finally \\(Pygoscelis\\) \\(adeliae\\), AKA the Adélie penguin.\nThe Palmer Penguins data set is a collection of physiological measurements for penguins collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, which is part of the Long Term Ecological Research Network. The data set contains measurements such as bill length, flipper length, and body mass for individuals from each of the three species of penguins: chinstrap, adelie, and gentoo. By analyzing these measurements, we can use various machine learning algorithms to classify each penguin into their respective species. This classification task can be performed by training a machine learning model on a portion of the data set and then using the trained model to predict the species of penguins in the remaining portion of the data set. By doing so, we can identify patterns and relationships in the data that can be used to accurately classify penguins into their respective species. We’ll decide later on which regression model to do the work. First lets import Pandas and the training data for use later.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nThe following code is from a machine learning pipeline where data preprocessing is performed using the scikit-learn library in Python.\nThe first line imports the LabelEncoder class which is used to encode categorical data. The next three lines create an instance of the LabelEncoder class, fits the encoder to the \\(Species\\) column of the training dataset, and creates a list of unique species names from the fitted encoder. The remaining code defines a function called \\(prepare\\)_\\(data\\) that drops certain columns, removes rows with missing or invalid data, encodes the \\(Species\\) column using the LabelEncoder’s transform() method, and performs one-hot encoding on the remaining features using the get_dummies() method. Finally, the function returns the preprocessed DataFrame and encoded \\(Species\\) values as separate arrays. The last two lines of code call the \\(prepare\\)_\\(data\\) function on the training dataset and outputs the preprocessed training columns.\n\nfrom sklearn.preprocessing import LabelEncoder # Import label encoder for tagging each data entry\n\nLabelEn = LabelEncoder() # creates an instance of the LabelEncoder class\nLabelEn.fit(train[\"Species\"]) # fits the LabelEncoder to the \"Species\" column of the train dataset using the fit() method\n\nspecies = [p.split()[0] for p in LabelEn.classes_] # creates a list called \"species\" that contains only the first word of each unique value in the \"Species\" column\n\ndef prepare_data(df): # defines a function called \"prepare_data\" that takes a DataFrame as input\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # drops several columns from the DataFrame using the drop() method\n    df = df[df[\"Sex\"] != \".\"] # drops any rows where the \"Sex\" column contains a period (\".\")\n    df = df.dropna() # drops any remaining rows that contain missing values\n    y = le.transform(df[\"Species\"]) # encodes the \"Species\" column using the LabelEncoder's transform() method\n    df = df.drop([\"Species\"], axis = 1) # one-hot encodes the remaining features of the DataFrame using the get_dummies() method\n    df = pd.get_dummies(df)\n    return df, y # returns the preprocessed DataFrame and the encoded \"Species\" values as separate arrays\n\nX_train, y_train = prepare_data(train)\nX_train # Output the preprocessed training columns\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nAnd above we’ve got a look at the dataframe we created, with all 14 columns of data points ranging from Culmen length to if they are male. Quickly though, let’s look at the context which classifies each of these three species into their respective groups.\n\n\n\n\nThe code below searches for the combination of categorical and numerical columns that achieve the highest accuracy score on the training data. The itertools library’s combinations function is used to generate all possible combinations of categorical and numerical columns from the all_qual_cols and all_quant_cols lists, respectively. The LogisticRegression class from scikit-learn is used to train a logistic regression model on each combination of columns, and the accuracy score of each model is compared to identify the combination that yields the highest score. The final result is printed out as a list of column names that produced the highest accuracy score on the training data. I decided to use logistic regression.\nLogistic regression is a good choice when the dependent variable is categorical and we want to predict the probability of a specific outcome. In this case, the dependent variable is categorical (i.e., the species of penguins), and we are trying to predict the probability of a penguin belonging to each of the three species.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom warnings import filterwarnings\nfilterwarnings('ignore') # Hiding the max iteration convergence warning\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", 'Island']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfinal_score = 0\nfinal_col = 0\n\nqual_idx = 0 # value used to index 'all_quant_cols' to get current value\n\nwhile qual_idx < len(all_qual_cols):\n    # loops through all combinations of categorical and numerical columns, \n    # and finally prints out the combination of columns that achieved the \n    # highest accuracy score on the training data.\n    qual = all_qual_cols[qual_idx]\n    qual_cols = [col for col in X_train.columns if qual in col ]\n    pair_idx = 0 # starts at 0 and is incremented by 1 until done\n    while pair_idx < len(all_quant_cols) - 1:\n        pair = (all_quant_cols[pair_idx], all_quant_cols[pair_idx + 1])\n        cols = list(pair) + qual_cols\n        LR = LogisticRegression()\n        LR.fit(X_train[cols], y_train)\n        if final_score < LR.score(X_train[cols], y_train):\n            final_col = cols\n            final_score = LR.score(X_train[cols], y_train)\n        pair_idx += 1\n    qual_idx += 1\n    \nprint(final_col)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Create a visualization\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\nplt.show()\n\n\n\n\nAbove you can see the graphs depicting three islands with penguins where we already know to which species each group belongs. Later we can compare how accurate our prediction is versus the actual data.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[final_col], y_train)\nLR.score(X_train[final_col], y_train)\n\n1.0\n\n\nEventually we end up with a score of 1.0, which is perfect!\nNext we can look how Culmen length and depth vary across species and across the islands.\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(min).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      34.5\n      13.1\n    \n    \n      Dream\n      32.1\n      15.5\n    \n    \n      Torgersen\n      33.5\n      16.6\n    \n  \n\n\n\n\n\ntrain[[\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby(\"Species\").mean().plot.bar(rot=20)\nplt.show(sns)\n\n\n\n\nWith info from the above graphs we can see the rough distrobutions across all three islands, and it looks like only Adelie penguins were on Torgersen island and the other two had Adelie and Gentoo or Adelie and Chinstraps.\n\nfrom matplotlib.patches import Patch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    # Get the first two columns of the input dataframe\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    \n    # Get the qualitative features of the input dataframe\n    qual_features = X.columns[2:]\n    \n    # Create a figure with subplots for each qualitative feature\n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # Create a grid to evaluate the model over\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    # Flatten the grid for easier evaluation\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      # Create a dataframe with all values set to 0\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      # Set the current qualitative feature to 1\n      for j in qual_features:\n        XY[j] = 0\n      XY[qual_features[i]] = 1\n\n      # Predict the class for each point in the grid and reshape the result\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      # Use a contour plot to visualize the decision regions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      # Get the data points that correspond to the current qualitative feature\n      ix = X[qual_features[i]] == 1\n      \n      # Plot the data points\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      # Set the axis labels\n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      # Create a legend for the different classes\n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      # Adjust the spacing between subplots\n      plt.tight_layout()\n\nThe above code defines a function named \\(plot\\)_\\(regions\\) that takes three arguments: model, \\(X\\), and \\(y\\). This function is intended to be used for plotting the decision boundaries of a classifier for a dataset with two continuous features and one or more categorical features.\nThe first few lines of the function extract the two continuous features (i.e., the first two columns) and the categorical features from the input data \\(X\\). Then, the function creates a grid of points spanning the range of the two continuous features, which is used for plotting the decision boundaries.\nThe main loop of the function iterates over each categorical feature in turn. For each feature, a new DataFrame \\(XY\\) is created that includes all combinations of the two continuous features and all possible values of the categorical feature (i.e., 0 or 1). The classifier model is then used to predict the class labels for each point in this grid, and the resulting predictions are plotted using a contour plot.\nWe will use this code to visualize decision boundaries of a classification model trained on the Palmer Penguins dataset, with different qualitative features.\n\n\n\n\nplt.close() # Clearing the old plot\nplot_regions(LR, X_train[final_col], y_train)\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nsns.relplot(\n    data=test,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\nplt.show(sns)\n\n\n\n\n\n\n\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[final_col], y_test)\n\n1.0\n\n\nAbove was a lot to take in, but in short we plot the decision boundary made by the \\(plot\\)_\\(regions\\) function and we can compare our decision with the actual labels from the original data. We got a score of \\(1.0\\) which means our model works on 100% of our penguin data points to predict which species they belong to based on culmen depth and length. We divided the penguins across three groups cutting two lines between them, labeling the Adelie red, Gentoo blue, and Chinstrap Green. Maybe now the next time you see one of these three species, you will be able to identify them!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning About Machine Learning",
    "section": "",
    "text": "Finn’s experiment to delve into the realm of unsupervised learning to explore its potential in compressing image data. Through systematic experimentation with singular value decomposition (SVD), Finn aims to uncover the effectiveness of this technique in reducing the size of image files.\n\n\n\n\n\n\nMay 3, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nTW: This post contains thoughts and ideas of the author and Dr. Timnit Gebru surrounding topics such at eugenics. Finn’s exploration of the ways AI and unregulated bias mitigation might negatively impact the future of the field and amplify inequality. Notes from the talk given by Dr. Timnit Gebru.\n\n\n\n\n\n\nApr 28, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s quick look at Timnit Gebru’s views on the current situation of ethics in AI.\n\n\n\n\n\n\nApr 19, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s attempt to classify three species of penguins.\n\n\n\n\n\n\nMar 28, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s implementation of the perceptron algorithm.\n\n\n\n\n\n\nMar 14, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s first blog post for CSCI 0451.\n\n\n\n\n\n\nFeb 15, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/unsupervised/unsupervised.html",
    "href": "posts/unsupervised/unsupervised.html",
    "title": "Looking at Image Compression",
    "section": "",
    "text": "In this blog post, I try to experiment with some machine learning approaches to compress some data.\nBut first we need some data, so I implemented the code below to grab us some from the internet. And specifically, we’re gonna need some Data, Crusher, Troi Picard, La Forge, Worf, and Riker.\n\nimport PIL, urllib\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(1701) # for the NCC-1701\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://static.wikia.nocookie.net/memoryalpha/images/2/23/Star_Trek_TNG_cast.jpg/revision/latest?cb=20200323081735&path-prefix=en\"\nimg = read_image(url)\nplt.imshow(img); plt.axis(\"off\")\nplt.savefig('TNG.png', pad_inches=0)\n\n\n\n\n\n# Set the figure size for the plot\nplt.figure(figsize=(15, 5))\n\n# Function to convert an image to grayscale as supplied from the blog docs\ndef to_greyscale(img):\n    return 1 - np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n\n# Convert the image to grayscale\ngrey_img = to_greyscale(img)\n\n# Plotting the original image\nplt.subplot(121)\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"original\")\n\n# Plotting the grayscale image\nplt.subplot(122)\nplt.imshow(grey_img, cmap=\"Greys\")\nplt.axis(\"off\")\nplt.title(\"greyscale\")\n\n# Save the figure as an image file\nplt.savefig('TNG.png', pad_inches=0)\n\n\n\n\n\n\nAs you can see, I converted the image to greyscale to help with the later process of compression.\nFor the sake of this exercise, it will be a lot easier to do with a bunch of single brightness values defining the difference from white to black than working with three separate color values.\nPlus it gives it a cool retro vibe :)\n\nNow let’s get into how I am going to compress this image. I am going to have Captain Picard explain:\n\n\n\n\nWe have made a remarkable discovery regarding the compression of grayscale images. Singular Value Decomposition (SVD) has proven to be a valuable tool in this endeavor. Allow me to elucidate the process:\nStep 1: Prepare the Image Prepare the grayscale image for compression. This involves converting it to a grayscale format, focusing solely on the shades of gray that shape its essence. The stellar code provided above has been implemented to do this for us.\nStep 2: Engage SVD Engage the extraordinary power of Singular Value Decomposition. SVD dissects the image into three significant components: U, Sigma, and V. Each element holds crucial information about the image’s structure.\nStep 3: Select the Singular Values Decisions must be made regarding the number of singular values to retain. This parameter, denoted as K, governs the balance between compression and image fidelity. Choose wisely, as the optimal K will define the outcome.\nStep 4: Precise Approximation Utilize the chosen K to create an approximation of the image. By retaining only the most significant singular values and their corresponding components, we achieve a compressed representation of the image while minimizing information loss.\nStep 5: Reconstruct the Image Reconstruct the compressed image by combining the retained singular values, their components, and the V matrix. Witness the image emerge, akin to the restoration of a cherished artifact.\nStep 6: Assess the Results Evaluate the compression achieved, considering storage requirements and image quality. Balance is key, for a successful compression strikes a harmonious chord between compactness and faithful representation.\nThrough this application of Singular Value Decomposition, we unveil a new avenue for grayscale image compression. This technique empowers us to reduce data while preserving the image’s essential characteristics. A remarkable voyage indeed.\n\n\n\n\nimport numpy as np\nimport warnings\n\ndef svd_reconstruct(img, num_sv=10, comp_factor=0, threshold=0):\n    # Get the dimensions of the image\n    m, n = img.shape\n    data = np.array(img)  # Convert the image to a numpy array\n\n    # Calculate the number of singular values (num_sv) if a compression factor (comp_factor) is specified\n    if comp_factor != 0:\n        num_sv = int((comp_factor * n * m) / (n + m + 1))\n\n    # Check if num_sv is larger than the dimensions of the image and adjust if necessary\n    if num_sv > m or num_sv > n:\n        warnings.warn(\"WARNING: num_sv does not fit dimensions of img\")\n        num_sv = min(n, m)\n\n    # Perform singular value decomposition (SVD) on the image data\n    U, sigma, V = np.linalg.svd(data)\n\n    # Create a diagonal matrix D with singular values\n    D = np.zeros_like(data, dtype=float)\n    D[:min(data.shape), :min(data.shape)] = np.diag(sigma)\n\n    # Determine the components for approximation based on whether a threshold is specified or not\n    if threshold == 0:\n        U_ = U[:, :num_sv]\n        D_ = D[:num_sv, :num_sv]\n        V_ = V[:num_sv, :]\n    else:\n        # Count the number of singular values larger than the threshold\n        new_num_sv = np.count_nonzero(D > threshold)\n\n        # Adjust new_num_sv if it exceeds the image dimensions\n        if new_num_sv > m or new_num_sv > n:\n            new_num_sv = min(n, m)\n\n        # Use new_num_sv if it is smaller than the specified components (num_sv)\n        if new_num_sv < num_sv:\n            num_sv = new_num_sv\n\n        U_ = U[:, :num_sv]\n        D_ = D[:num_sv, :num_sv]\n        V_ = V[:num_sv, :]\n\n    # Reconstruct the image using the selected components\n    data_ = U_ @ D_ @ V_\n\n    # Calculate the storage size and compression ratio\n    storage = (m * num_sv + num_sv + num_sv * n)\n    fraction = storage / (n * m)\n\n    return data_, round(fraction * 100, 2)\n\nI implemented the code above using the examples provided in blog post documentation and help hours. I call the K constant “num_sv” as it represents the number of singular values to be retained and used in our compression factors. Otherwise, the comments are pretty self explanatory, where the svd_reconstruct function takes a grayscale image and performs singular value decomposition on it. It allows for compression of the image by retaining a specified number of singular values or by applying a threshold to select significant singular values, and returns the reconstructed image along with the storage size and compression ratio. Next let’s look at a quick comparison.\n\ndef compare_svd_reconstruct(img, num_sv=10, comp_factor=0, threshold=0):\n    # Convert the input image to a numpy array\n    A = np.array(img)\n\n    # Reconstruct the image using singular value decomposition (SVD)\n    A_, storage_percent = svd_reconstruct(A, num_sv, comp_factor=comp_factor, threshold=threshold)\n\n    # Get the dimensions of the original image\n    m, n = A.shape\n\n    # Create a figure with two subplots for comparing the original and reconstructed images\n    fig, axarr = plt.subplots(1, 2, figsize=(18, 9))\n\n    # Display the original image in the first subplot\n    axarr[0].imshow(A, cmap=\"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title=\"original greyscale image\")\n\n    # Display the reconstructed image in the second subplot\n    axarr[1].imshow(A_, cmap=\"Greys\")\n    axarr[1].axis(\"off\")\n    img_title = \"reconstructed image\\n\" + str(storage_percent) + \"% storage\"\n    axarr[1].set(title=img_title)\n\n\ncompare_svd_reconstruct(to_greyscale(img), 50)\n\n\n\n\n\ncompare_svd_reconstruct(to_greyscale(img), 50)\n\nWe can see here that this number of components used to compress the image (50) is probably not enough. So let’s run some experiments to see what might be a good number of singular values for our compression.\n\ndef svd_experiment(img):\n    rows = 5\n    columns = 3\n\n    # Create a figure with subplots for displaying the results\n    fig, axarr = plt.subplots(rows, columns, figsize=(20, 25))\n\n    # Iterate over the rows and columns of the subplots\n    for i in range(rows):\n        for j in range(columns):\n            # Determine the number of components (k) based on the current row and column\n            k = (i * 4 + j + 1) * 15\n\n            # Reconstruct the image using the specified number of components\n            A_, storage_percent = svd_reconstruct(img, k)\n\n            # Create a title for the subplot indicating the number of components and storage percentage\n            img_title = str(k) + \" components\\n\" + str(storage_percent) + \"% storage\"\n\n            # Display the reconstructed image in the current subplot\n            axarr[i][j].imshow(A_, cmap=\"Greys\")\n            axarr[i][j].axis(\"off\")\n            axarr[i][j].set(title=img_title)\n\n    # Adjust the spacing between subplots\n    fig.tight_layout()\n\n\nsvd_experiment(to_greyscale(img))\nplt.savefig('experiment.png')\n\n\n\n\nNow we’re getting somewhere. The first few tests clearly have too few SVs, but by the 2nd row, much of the image is already there. And by the 3rd or 4th row, the compression artifacts created by our method are much less distracting and the image is difficult to discern from the original without pixel peeping. This is particularly noticeable on the crew’s clothing, where the compression has a hard time doing smooth gradients. By around 225 components would be where I draw the line:\n\ncompare_svd_reconstruct(to_greyscale(img), 225)\n\n\n\n\nIndeed, by utilizing singular value decomposition (SVD) for image compression, we can significantly reduce the size of the image while preserving its essential features. This method works well for compressing this image of TNG crew, but we may have to tweak it in the future for other uses. In my estimate, achieving a compression down to approximately 23% of the original size demonstrates the effectiveness of SVD-based compression in reducing storage requirements without sacrificing essential image quality."
  }
]