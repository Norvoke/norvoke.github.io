[
  {
    "objectID": "posts/ololi/ololi.html",
    "href": "posts/ololi/ololi.html",
    "title": "Ololi, and me trying to make it on to the Apple App Store…",
    "section": "",
    "text": "Chapter 1: “It Begins with an Emoji…”\nArmed with SwiftUI, a penchant for whimsy, and an inexplicable fondness for fruit emojis, I embarked on creating “ololi,” the emoji matching game no one knew they needed until now. Here’s the gist: you’ve got a bunch of fruit emojis, and your mission, should you choose to accept it, is to match ’em up before time runs out. Sounds simple, right? Oh, sweet summer child…\n\nimport SwiftUI\n\nstruct ContentView: View {\n    @State private var emojiCards = [\"🍎\", \"🍌\", \"🍇\", \"🍉\", \"🍓\", \"🍒\", \"🍑\", \"🍍\"].doubled().shuffled()\n    @State private var flippedCards = Set<Int>()\n    @State private var matchedCards = Set<Int>()\n\nFirst off, doubling and shuffling an array of emojis? Check. It’s like making a smoothie but forgetting to put the lid on the blender.\nChapter 2: “The Flippening”\nKeeping track of which cards have been flipped and which have found their soulmate was a bit like trying to remember where I left my keys. In a fit of what I can only describe as “coding grace,” I managed to wrangle some state variables into submission.\n\n    @State private var gameTimer = Timer.publish(every: 1, on: .main, in: .common).autoconnect()\n    @State private var timeRemaining = 90\n    @State private var isGameOver = false\n    @State private var showingTitleScreen = true // Control the display of the title screen\n    @State private var selectedTime = 90 // Default to easy mode\n\nIntroducing a ticking time bomb into the mix because, hey, life’s not exciting enough as it is.\nChapter 3: “A Wild UI Appears”\nBuilding the UI was like deciding on an outfit. You start simple, then before you know it, you’re layering like it’s Fashion Week. GeometryReader? Check. VStacks within VStacks? Check. A sense of existential dread as you wonder if anyone will actually play this game? Double-check.\n\nvar body: some View {\n        GeometryReader { geometry in\n            VStack {\n                if showingTitleScreen {\n                    TitleScreenView(startGame: {\n                        self.showingTitleScreen = false\n                        self.timeRemaining = self.selectedTime\n                    }, selectedTime: $selectedTime)\n                } else {\n                    GameTitleView() // Display the game title\n                    if isGameOver {\n                        GeometryReader { geo in\n                            gameOverView\n                                .frame(width: geo.size.width, height: geo.size.height)\n                        }\n                    } else {\n                        gameView\n                    }\n                }\n            }\n\nI crafted a title screen with the same care and attention a cat gives to knocking things off a table. Effortlessly, obviously.\nChapter 4: “Game Over, Man. Game Over!”\nThe game had to end somehow, and not just with me questioning my life choices. So, a game-over screen was born, popping up with all the gentleness of a jack-in-the-box set to a timer.\n\nprivate var gameOverView: some View {\n        VStack {\n            Text(\"Game Over\")\n                .font(.largeTitle)\n            Text(\"You matched all the cards with \\(timeRemaining) seconds remaining!\")\n                .font(.headline)\n            Button(\"Play Again\") {\n                resetGame()\n            }\n            .padding()\n            .background(Color.blue)\n            .foregroundColor(.white)\n            .cornerRadius(10)\n        }\n    }\n\nAnd, like any good gamer, I included a “Play Again” button because, let’s face it, we’re all gluttons for punishment.\nEpilogue: “Reflections and Regrets”\nLooking back, creating “ololi” was a journey of self-discovery, frustration, and an absurd amount of Googling. Would I do it again? In a heartbeat. After all, there’s something magical about bringing a bunch of pixels to life and watching as they frustrate and delight people in equal measure.\nSo, there you have it. My spring break, encapsulated in a game about matching fruit emojis. Who needs internships when you have SwiftUI and a stubborn refusal to do anything remotely productive?\nWell me… I need an internship. And hopefully, despite my frustration with how Xcode differs from literally every other development platform I have used, this experience has taught me something about software development. I hope to keep working on this app, as it is far from perfect.\nIt’s almost human in that way… Too human…\nIn all seriousness this project has taught me some useful skills I hope to use in future schemes, hopefully incorporating my earlier Russian Verb ML Classifier model. Of course I would likely have to tackle Apple’s CreateML tool, which will be a tale for another day…"
  },
  {
    "objectID": "posts/unsupervised/unsupervised.html",
    "href": "posts/unsupervised/unsupervised.html",
    "title": "Looking at Image Compression",
    "section": "",
    "text": "In this blog post, I try to experiment with some machine learning approaches to compress some data.\nBut first we need some data, so I implemented the code below to grab us some from the internet. And specifically, we’re gonna need some Data, Crusher, Troi Picard, La Forge, Worf, and Riker.\n\nimport PIL, urllib\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.random.seed(1701) # for the NCC-1701\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\nurl = \"https://static.wikia.nocookie.net/memoryalpha/images/2/23/Star_Trek_TNG_cast.jpg/revision/latest?cb=20200323081735&path-prefix=en\"\nimg = read_image(url)\nplt.imshow(img); plt.axis(\"off\")\nplt.savefig('TNG.png', pad_inches=0)\n\n\n\n\n\n# Set the figure size for the plot\nplt.figure(figsize=(15, 5))\n\n# Function to convert an image to grayscale as supplied from the blog docs\ndef to_greyscale(img):\n    return 1 - np.dot(img[..., :3], [0.2989, 0.5870, 0.1140])\n\n# Convert the image to grayscale\ngrey_img = to_greyscale(img)\n\n# Plotting the original image\nplt.subplot(121)\nplt.imshow(img)\nplt.axis(\"off\")\nplt.title(\"original\")\n\n# Plotting the grayscale image\nplt.subplot(122)\nplt.imshow(grey_img, cmap=\"Greys\")\nplt.axis(\"off\")\nplt.title(\"greyscale\")\n\n# Save the figure as an image file\nplt.savefig('TNG.png', pad_inches=0)\n\n\n\n\n\n\nAs you can see, I converted the image to greyscale to help with the later process of compression.\nFor the sake of this exercise, it will be a lot easier to do with a bunch of single brightness values defining the difference from white to black than working with three separate color values.\nPlus it gives it a cool retro vibe :)\n\nNow let’s get into how I am going to compress this image. I am going to have Captain Picard explain:\n\n\n\n\nWe have made a remarkable discovery regarding the compression of grayscale images. Singular Value Decomposition (SVD) has proven to be a valuable tool in this endeavor. Allow me to elucidate the process:\nStep 1: Prepare the Image Prepare the grayscale image for compression. This involves converting it to a grayscale format, focusing solely on the shades of gray that shape its essence. The stellar code provided above has been implemented to do this for us.\nStep 2: Engage SVD Engage the extraordinary power of Singular Value Decomposition. SVD dissects the image into three significant components: U, Sigma, and V. Each element holds crucial information about the image’s structure.\nStep 3: Select the Singular Values Decisions must be made regarding the number of singular values to retain. This parameter, denoted as K, governs the balance between compression and image fidelity. Choose wisely, as the optimal K will define the outcome.\nStep 4: Precise Approximation Utilize the chosen K to create an approximation of the image. By retaining only the most significant singular values and their corresponding components, we achieve a compressed representation of the image while minimizing information loss.\nStep 5: Reconstruct the Image Reconstruct the compressed image by combining the retained singular values, their components, and the V matrix. Witness the image emerge, akin to the restoration of a cherished artifact.\nStep 6: Assess the Results Evaluate the compression achieved, considering storage requirements and image quality. Balance is key, for a successful compression strikes a harmonious chord between compactness and faithful representation.\nThrough this application of Singular Value Decomposition, we unveil a new avenue for grayscale image compression. This technique empowers us to reduce data while preserving the image’s essential characteristics. A remarkable voyage indeed.\n\n\n\n\nimport numpy as np\nimport warnings\n\ndef svd_reconstruct(img, num_sv=10, comp_factor=0, threshold=0):\n    # Get the dimensions of the image\n    m, n = img.shape\n    data = np.array(img)  # Convert the image to a numpy array\n\n    # Calculate the number of singular values (num_sv) if a compression factor (comp_factor) is specified\n    if comp_factor != 0:\n        num_sv = int((comp_factor * n * m) / (n + m + 1))\n\n    # Check if num_sv is larger than the dimensions of the image and adjust if necessary\n    if num_sv > m or num_sv > n:\n        warnings.warn(\"WARNING: num_sv does not fit dimensions of img\")\n        num_sv = min(n, m)\n\n    # Perform singular value decomposition (SVD) on the image data\n    U, sigma, V = np.linalg.svd(data)\n\n    # Create a diagonal matrix D with singular values\n    D = np.zeros_like(data, dtype=float)\n    D[:min(data.shape), :min(data.shape)] = np.diag(sigma)\n\n    # Determine the components for approximation based on whether a threshold is specified or not\n    if threshold == 0:\n        U_ = U[:, :num_sv]\n        D_ = D[:num_sv, :num_sv]\n        V_ = V[:num_sv, :]\n    else:\n        # Count the number of singular values larger than the threshold\n        new_num_sv = np.count_nonzero(D > threshold)\n\n        # Adjust new_num_sv if it exceeds the image dimensions\n        if new_num_sv > m or new_num_sv > n:\n            new_num_sv = min(n, m)\n\n        # Use new_num_sv if it is smaller than the specified components (num_sv)\n        if new_num_sv < num_sv:\n            num_sv = new_num_sv\n\n        U_ = U[:, :num_sv]\n        D_ = D[:num_sv, :num_sv]\n        V_ = V[:num_sv, :]\n\n    # Reconstruct the image using the selected components\n    data_ = U_ @ D_ @ V_\n\n    # Calculate the storage size and compression ratio\n    storage = (m * num_sv + num_sv + num_sv * n)\n    fraction = storage / (n * m)\n\n    return data_, round(fraction * 100, 2)\n\nI implemented the code above using the examples provided in blog post documentation and help hours. I call the K constant “num_sv” as it represents the number of singular values to be retained and used in our compression factors. Otherwise, the comments are pretty self explanatory, where the svd_reconstruct function takes a grayscale image and performs singular value decomposition on it. It allows for compression of the image by retaining a specified number of singular values or by applying a threshold to select significant singular values, and returns the reconstructed image along with the storage size and compression ratio. Next let’s look at a quick comparison.\n\ndef compare_svd_reconstruct(img, num_sv=10, comp_factor=0, threshold=0):\n    # Convert the input image to a numpy array\n    A = np.array(img)\n\n    # Reconstruct the image using singular value decomposition (SVD)\n    A_, storage_percent = svd_reconstruct(A, num_sv, comp_factor=comp_factor, threshold=threshold)\n\n    # Get the dimensions of the original image\n    m, n = A.shape\n\n    # Create a figure with two subplots for comparing the original and reconstructed images\n    fig, axarr = plt.subplots(1, 2, figsize=(18, 9))\n\n    # Display the original image in the first subplot\n    axarr[0].imshow(A, cmap=\"Greys\")\n    axarr[0].axis(\"off\")\n    axarr[0].set(title=\"original greyscale image\")\n\n    # Display the reconstructed image in the second subplot\n    axarr[1].imshow(A_, cmap=\"Greys\")\n    axarr[1].axis(\"off\")\n    img_title = \"reconstructed image\\n\" + str(storage_percent) + \"% storage\"\n    axarr[1].set(title=img_title)\n\n\ncompare_svd_reconstruct(to_greyscale(img), 50)\n\n\n\n\n\ncompare_svd_reconstruct(to_greyscale(img), 50)\n\nWe can see here that this number of components used to compress the image (50) is probably not enough. So let’s run some experiments to see what might be a good number of singular values for our compression.\n\ndef svd_experiment(img):\n    rows = 5\n    columns = 3\n\n    # Create a figure with subplots for displaying the results\n    fig, axarr = plt.subplots(rows, columns, figsize=(20, 25))\n\n    # Iterate over the rows and columns of the subplots\n    for i in range(rows):\n        for j in range(columns):\n            # Determine the number of components (k) based on the current row and column\n            k = (i * 4 + j + 1) * 15\n\n            # Reconstruct the image using the specified number of components\n            A_, storage_percent = svd_reconstruct(img, k)\n\n            # Create a title for the subplot indicating the number of components and storage percentage\n            img_title = str(k) + \" components\\n\" + str(storage_percent) + \"% storage\"\n\n            # Display the reconstructed image in the current subplot\n            axarr[i][j].imshow(A_, cmap=\"Greys\")\n            axarr[i][j].axis(\"off\")\n            axarr[i][j].set(title=img_title)\n\n    # Adjust the spacing between subplots\n    fig.tight_layout()\n\n\nsvd_experiment(to_greyscale(img))\nplt.savefig('experiment.png')\n\n\n\n\nNow we’re getting somewhere. The first few tests clearly have too few SVs, but by the 2nd row, much of the image is already there. And by the 3rd or 4th row, the compression artifacts created by our method are much less distracting and the image is difficult to discern from the original without pixel peeping. This is particularly noticeable on the crew’s clothing, where the compression has a hard time doing smooth gradients. By around 225 components would be where I draw the line:\n\ncompare_svd_reconstruct(to_greyscale(img), 225)\n\n\n\n\nIndeed, by utilizing singular value decomposition (SVD) for image compression, we can significantly reduce the size of the image while preserving its essential features. This method works well for compressing this image of TNG crew, but we may have to tweak it in the future for other uses. In my estimate, achieving a compression down to approximately 23% of the original size demonstrates the effectiveness of SVD-based compression in reducing storage requirements without sacrificing essential image quality."
  },
  {
    "objectID": "posts/gebru-post/gebru.html",
    "href": "posts/gebru-post/gebru.html",
    "title": "Timnit Gebru and Modern Ethics in AI",
    "section": "",
    "text": "Introducing Dr. Timnit Gebru, the computer scientist extraordinaire! She’s like the Hermione Granger of the AI world, except she’s not fictional (and she’s probably better at coding).\nDr. Gebru is a superstar when it comes to algorithmic bias and data mining. She’s the brains behind the Gender Shades study, which exposed the facial recognition software’s tendency to overlook black women (seriously, what’s up with that?). She’s also the co-founder of Black in AI, an organization that’s on a mission to get more black individuals involved in AI. You go, girl!\n\n\n\n\n\n\nNote\n\n\n\n“technology is mostly being used to target the very people who are most vulnerable” Gebru in her 2020 talk Computer vision in practice: who is benefiting and who is being harmed?\n\n\nDr. Gebru is not just smart, she’s also sassy. She doesn’t shy away from speaking up about the ethical and social implications of AI. In fact, according to her Wikipedia page (which is known for its authentic journalism), she left her job at Google because they wanted her to take her unpublished paper and shove it where the sun don’t shine. (Just kidding, they just wanted her to remove some names. But still, it was a big deal.)\nSpeaking of ethical concerns, let’s talk about ChatGPT. Middlebury College, like many other institutions, has yet to establish a campus-wide policy regarding the use of ChatGPT and other AI tools. This means that professors are free to use ChatGPT if they want to, which is kind of like giving your students a genie in a bottle (except instead of wishes, they get answers to their essays).\nHowever, ChatGPT comes with its own set of problems. For example, the premium version gives students who can afford it an unfair advantage. And let’s not forget that ChatGPT is trained on a whole lot of language data, which includes some pretty nasty stuff. Developers try to filter out the biased stuff, but it still seeps through. Just like that one drop of ketchup that always ends up on your white shirt.\nSo what did Dr. Gebru have to say about all of this? Well, she talked about the reliability (or lack thereof) of computer vision technology. Turns out, those facial recognition tools aren’t as accurate as we thought. In fact, they’re pretty biased. And when they’re used to judge people’s personalities and emotions (yes, that’s a thing), it perpetuates structural racism. Oh, and don’t get her started on Faception, which singles out people from marginalized groups as terrorists. Not cool, Faception.\nDr. Gebru also talked about the lack of diversity in datasets. It’s a big problem because it leads to a high disparity in facial recognition results. And even when companies do try to gather diverse information, they end up doing it unethically. It’s like they’re playing Pokemon, except instead of catching them all, they’re catching all the darker skin images and scraping images of transgender YouTubers without their consent.\nBut here’s the thing: technology is not always used in the way it’s designed. Sometimes it leads to more discrimination and problems than efficiency. So Dr. Gebru suggested that we need to think beyond diversity in datasets and consider structural and real representation. We need to make sure that panels aren’t just filled with people from dominant groups and those closest to money. Because fairness isn’t just about math, it’s about society.\nSo, what do you think, as some questions for the reader: Should we prohibit or regulate the use of facial recognition on people? And how can computer vision technology help marginalized communities? We’re all ears!"
  },
  {
    "objectID": "posts/post-2023-02-15/index.html",
    "href": "posts/post-2023-02-15/index.html",
    "title": "First Blog Post",
    "section": "",
    "text": "This is my first post to my official blog for CSCI 0451. I am testing out some things and will be working on implementing the perceptron algorithm in python to draw a line between two clouds of points."
  },
  {
    "objectID": "posts/post-2023-02-15/index.html#math",
    "href": "posts/post-2023-02-15/index.html#math",
    "title": "First Blog Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/approach/approach.html",
    "href": "posts/approach/approach.html",
    "title": "On the Limits of the Quantitative Approach to Bias and Fairness",
    "section": "",
    "text": "— Narayanan (2022)"
  },
  {
    "objectID": "posts/approach/approach.html#introduction",
    "href": "posts/approach/approach.html#introduction",
    "title": "On the Limits of the Quantitative Approach to Bias and Fairness",
    "section": "Introduction",
    "text": "Introduction\nThe use of quantitative methods for assessing discrimination and bias has gained increasing attention in recent years. While some scholars argue that quantitative methods are effective in identifying and addressing discrimination, others, like Narayanan (2022), contend that they serve to justify the status quo and do more harm than good. In this essay, I will examine Narayanan’s position and engage with a number of scholarly sources to discuss the uses, benefits, and downsides of quantitative methods. I will also present my argument and position on Narayanan’s claim.\n\nNarayanan’s Position\nNarayanan (2022) argues that quantitative methods for assessing discrimination and bias are primarily used to justify the status quo, and do more harm than good. He posits that statistical models and algorithms are not neutral, but rather reflect the values and assumptions of their creators. As a result, when quantitative methods are used to study social phenomena, they tend to perpetuate existing power structures and social hierarchies. For instance, machine learning algorithms used in hiring or loan approval processes may be biased against certain groups, reflecting the historical discrimination and inequality that exist in society. According to Narayanan, relying on quantitative methods risks reinforcing these existing power structures, rather than challenging them.\n\n\nThe Uses and Benefits of Quantitative Methods\nWhile Narayanan argues that quantitative methods do more harm than good, other scholars have identified various uses and benefits of quantitative methods. One benefit is that statistical models and machine learning algorithms can identify and address disparities in healthcare outcomes. Landon et al. (2003) used statistical methods to show that African American patients are less likely to receive appropriate treatment for cardiovascular disease than white patients. They “conclude that important technical barriers stand in the way of using physician clinical performance assessment for evaluating the competency of individual physicians.” This research has led to efforts to address the underlying causes of these disparities, such as unequal access to healthcare and implicit bias among healthcare providers.\nSimilarly, Pager and Shepherd (2008) used statistical methods to evaluate the impact of affirmative action policies on employment outcomes. They “provide an overview of major findings from studies of discrimination in each of the four domains [employment, housing, credit markets, and consumer interactions]; and, finally, [they] turn to a discussion of the individual, organizational, and structural mechanisms that may underlie contemporary forms of discrimination.” Their research can help policymakers design more effective policies that promote social justice and reduce discrimination.\nAdditionally, Caliskan, Bryson, and Narayanan (2017) developed methods for auditing machine learning algorithms to identify sources of bias and discrimination, ensuring that algorithms are fair and unbiased. They showed “that applying machine learning to ordinary human language results in human-like semantic biases,” meaning data could be analyzed for bias. Narayanan was actually one of the authors in this paper, which shows how this might have influenced his opinions.\nLastly, Crooks and Heppenstall (2012) also suggest that quantitative methods can be used to analyze and understand complex urban systems. They argue that these methods can contribute to the development of more inclusive and sustainable cities, stating that it “is powerful paradigm that holds great promise for facilitating greater understanding of geographical systems.” They suggest that quantitative methods can be combined with qualitative methods to provide a more comprehensive understanding of urban phenomena.\n\n\nMore Support\nDiving deeper into their study (Pager and Shepherd (2008)) on discrimination in hiring, Pager and Shepherd found how affirmative action policies resulted in significant improvements in the employment outcomes of African American and Latino candidates. The policies helped to reduce discrimination in hiring and promote social justice. Similarly, Smith and Hasan (2020) argues that quantitative methods can be used to promote fairness in algorithmic decision-making. In their research, it had far-reaching concequences in psychiatric patients whose background could be well understood by their model. By defining fairness mathematically and incorporating it into algorithms, we can reduce bias and promote social justice.\n\nIn their notable investigation, Dressel and Farid (2018) scrutinized the accuracy of COMPAS, a widely utilized commercial algorithm employed in the realm of criminal justice decision-making. The study brought to light a troubling revelation: the algorithm exhibited bias against African American defendants, as they were disproportionately categorized as high-risk compared to their white counterparts. This finding not only raises significant concerns about the fairness and equity of the criminal justice system but also underscores the urgent necessity for enhanced transparency and accountability in algorithmic decision-making.\n\n\nThe ramifications of biased algorithms in criminal justice decision-making are profound. The overrepresentation of African American defendants classified as high-risk could potentially lead to harsher sentences, increased pretrial detention, and a perpetuation of existing racial disparities within the system. Such biased outcomes deepen social inequalities and undermine public trust in the fairness of the justice system.\n\n\nTo address these challenges, it is imperative to establish greater transparency in algorithmic decision-making processes. This entails a comprehensive understanding of how algorithms are developed, the data they are trained on, and the potential biases embedded within them. The lack of transparency surrounding proprietary algorithms, like COMPAS, has raised concerns about the potential opacity and inscrutability of these systems.\n\n\nAccountability is equally crucial. There should be mechanisms in place to assess the performance and impact of algorithms in real-world contexts. This includes conducting regular audits, evaluating the accuracy and fairness of algorithmic outcomes, and addressing any identified biases promptly. Establishing clear guidelines and standards for the use of algorithms in the criminal justice system can help mitigate the potential harm caused by biased decision-making.\n\nLamont and Molnár (2002) suggest that quantitative methods can reinforce existing power structures and ideologies. They argue that quantitative methods tend to prioritize “objective” and “scientific” knowledge, which is often based on dominant cultural assumptions and perspectives. This approach can lead to the exclusion of diverse perspectives and the marginalization of underrepresented groups.\nFinally, Blodgett et al. (2020) argue that quantitative methods can be used to address historical inequalities and promote social justice. They argue that by collecting and analyzing data on the experiences of marginalized groups, we can identify and address systemic inequalities. This approach can help to promote social justice and reduce discrimination.\n\n\nArgument and Position\nIn conclusion, while Narayanan (2022) argues that quantitative methods do more harm than good, the scholarly sources reviewed in this essay demonstrate that quantitative methods can be effective in identifying and addressing discrimination. While there are instances where quantitative methods may serve to justify the status quo and perpetuate existing power structures, they can also be used to promote social justice and reduce discrimination. The key, like for many things in life, is to take into account all factors. This is easier said than done.\n\n\nConclusion\nNarayanan’s argument that quantitative methods are primarily used to justify the status quo and do more harm than good is a valid concern. However, it does not mean that quantitative methods should not be used in the analysis of discrimination and bias. Rather, it highlights the need for a critical examination of the way these methods are used and the context in which they are applied.\nQuantitative methods are a valuable tool in uncovering patterns of discrimination and bias that may be hidden in complex data. As a result, they can be used to hold institutions and individuals accountable for their actions. However, the analysis of discrimination and bias should not be reduced to a simple numbers game, where the goal is to achieve statistical parity. It is important to recognize that discrimination and bias are systemic issues that require a more comprehensive approach.\nTherefore, it is essential to consider the limitations of quantitative methods and to use them in conjunction with qualitative methods and other forms of analysis to gain a more holistic understanding of the problem. In doing so, we can ensure that we are not only identifying patterns of discrimination and bias but also developing effective strategies to address them.\nIn conclusion, Narayanan’s assertion that quantitative methods can do more harm than good is an important reminder that we need to be mindful of the ways in which we use these methods. While quantitative methods can be a valuable tool in uncovering patterns of discrimination and bias, they are not a panacea. To fully understand and address discrimination and bias, we need to take a more comprehensive approach that includes both quantitative and qualitative methods, as well as a critical examination of the social and political contexts in which discrimination and bias occur."
  },
  {
    "objectID": "posts/project/project.html",
    "href": "posts/project/project.html",
    "title": "A Russian Verb Classification Model trained on the Russian Open Corpora dataset",
    "section": "",
    "text": "This project aimed to develop a Russian Verb Classification Model capable of accurately predicting attributes such as tense, person, mood, etc., when given a Russian verb or infinitive. By utilizing the Random Forest Classifier model and leveraging the data from the Open Corpora dataset, my goal was to achieve a high level of accuracy that could rival traditional dictionaries. Through meticulous implementation and evaluation, I successfully produced a model that delivers a list of likely attributes with precision comparable to dictionaries, empowering users with fast and reliable verb attribute predictions."
  },
  {
    "objectID": "posts/project/project.html#the-data",
    "href": "posts/project/project.html#the-data",
    "title": "A Russian Verb Classification Model trained on the Russian Open Corpora dataset",
    "section": "The Data:",
    "text": "The Data:\n\n\n\nThe homepage for the Russian OpenCorpora project translated (using Google translate) into English.\n\n\n\nThe OpenCorpora dataset is a valuable resource for linguistic analysis of the Russian language. It was collected and encoded by Rachael Tatman, and the dataset consists of two files: the corpus and the dictionary. The corpus is provided in .json format, while the dictionary is in plain text. The dataset is encoded in UTF-8, ensuring compatibility with various systems and applications.\nThe example snippet from the dataset demonstrates the format of the corpus. Each row represents a word or verb form along with its associated tags, providing information about the word’s part of speech, animacy, gender, number, case, aspect, person, tense, and mood. For nouns, the tags include information about animacy, gender, number, and case. For verbs, the tags include details about aspect, person, tense, and mood.\n\nExample entries:\n1:  ЁЖ    NOUN,anim,masc sing,nomn  ЕЖА    NOUN,anim,masc sing,gent  ЕЖУ    NOUN,anim,masc sing,datv  ЕЖА    NOUN,anim,masc sing,accs  ЕЖОМ    NOUN,anim,masc sing,ablt  ЕЖЕ    NOUN,anim,masc sing,loct  ЕЖИ    NOUN,anim,masc plur,nomn  ЕЖЕЙ    NOUN,anim,masc plur,gent  ЕЖАМ    NOUN,anim,masc plur,datv  …  41:  ЁРНИЧАЮ    VERB,impf,intr sing,1per,pres,indc  ЁРНИЧАЕМ    VERB,impf,intr plur,1per,pres,indc  ЁРНИЧАЕШЬ    VERB,impf,intr sing,2per,pres,indc  ЁРНИЧАЕТЕ    VERB,impf,intr plur,2per,pres,indc  ЁРНИЧАЕТ    VERB,impf,intr sing,3per,pres,indc  ЁРНИЧАЮТ    VERB,impf,intr plur,3per,pres,indc  ЁРНИЧАЛ    VERB,impf,intr masc,sing,past,indc  ЁРНИЧАЛА    VERB,impf,intr femn,sing,past,indc  ЁРНИЧАЛО    VERB,impf,intr neut,sing,past,indc  ЁРНИЧАЛИ    VERB,impf,intr plur,past,indc  ЁРНИЧАЙ    VERB,impf,intr sing,impr,excl  ЁРНИЧАЙТЕ    VERB,impf,intr plur,impr,excl …\nIt is important to note some potential limitations of the OpenCorpora dataset. The dataset primarily focuses on Russian nouns and verbs, with limited coverage of other parts of speech. While it provides a significant amount of linguistic data with 1.5 million words, it may not represent the entirety of the Russian language. Additionally, the dataset’s representation of linguistic phenomena, such as dialectal variations or specific domains, may be limited.\nFurthermore, the OpenCorpora dataset represents written Russian language usage, which may differ from spoken language or informal registers. The dataset may not capture the full range of variation and contextual nuances present in the language. The dataset is also subject to the bias of its sources, which range from across the internet to various publications and print media, which are not listed in full on their documentation. It is necesary to understand these sources are subject to the regional restrictions on freedom of print and other contributing factors which may alter the way written language differs from the de facto. It is essential to consider these limitations when utilizing the dataset for linguistic analysis or developing models based on its contents."
  },
  {
    "objectID": "posts/project/project.html#my-approach",
    "href": "posts/project/project.html#my-approach",
    "title": "A Russian Verb Classification Model trained on the Russian Open Corpora dataset",
    "section": "My Approach:",
    "text": "My Approach:\nInitially I needed to pull only the verbs and infinitives from the dataset, so I wrote a short program to parse the text file and read the data into a pandas data frame. It just looked at the first attribute in the text line whether it was ‘INFN’ or ‘VERB’.\n\nimport pandas as pd\n\n# Read dictionary.txt file\nwith open('dictionary.txt', 'r', encoding='utf-8') as f:\n    data = f.readlines()\n\n# Extract rows with \"VERB\" or \"INFN\"\nverb_inf_data = []\nfor i, row in enumerate(data):\n    if \"VERB\" in row or \"INFN\" in row:\n        verb_inf_data.append(row.strip().split(\"\\t\"))\n\n# Create pandas dataframe\ndf = pd.DataFrame(verb_inf_data[1:], columns=[\"verb\", \"attributes\"])\n\nWhich produced a data frame like this:\nx  verb    attributes  0  ЁЖИМ    VERB,impf,tran plur,1per,pres,indc 1  ЁЖИШЬ   VERB,impf,tran sing,2per,pres,indc 2  ЁЖИТЕ   VERB,impf,tran plur,2per,pres,indc 3  ЁЖИТ    VERB,impf,tran sing,3per,pres,indc 4  ЁЖАТ    VERB,impf,tran plur,3per,pres,indc ...    ... ... 451441 ВЛОЖАТСЯ    VERB,perf,intr plur,3per,futr,indc 451442 ВЛОЖИМСЯ    VERB,perf,intr sing,impr,incl 451443 ВЛОЖИМТЕСЬ  VERB,perf,intr plur,impr,incl 451444 ВЛОЖИСЬ VERB,perf,intr sing,impr,excl 451445 ВЛОЖИТЕСЬ   VERB,perf,intr plur,impr,excl 451446 rows × 2 columns\nThen I needed to break up the data frame into parts, where the verb would go first after an entry number, followed by the list of attributes:\n\n# Add leading numbers to each entry\ndf.index = df.index + 1\ndf.index.name = 'Number'\n\nsplit_data = []\nfor i, row in verb_inf_data:\n    split_data.append((row.strip().split(\",\")))\n\ndf = pd.DataFrame({'Entry': verb_inf_data[1:]})\n\nverb_frame = pd.DataFrame({'verb': list(list(zip(*verb_inf_data))[0])})\nverb_frame.insert(1, \"attributes\", split_data)\n\nThis is where my data parsing would produce some issues down the line in my training section, as to the computer, the attributes entries, while seperated by comma, were still treated as whole parts instead of individial attributes.\nInitially I tried KMeans clustering, as in my brain, I had an idea that there were many groups of verbs, like all those which are plural or all those that are first person, and that the model could pull these out. I also perfomed the vectorization step here with the dividing of the data into valadation and test portions:\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Load the DataFrame from your source\ndata = df.head(2000)\n\n# Split the data into input features (verbs) and target labels (attributes)\nX = data['verb']\ny = data['attributes']\n\n# Vectorize input features using CountVectorizer\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(X)\nself._vectorizer = vectorizer\n\nKMeans(n_clusters=20, random_state=0)\n\n# Evaluate the clustering using silhouette score\nlabels = model.predict(X)\nscore = silhouette_score(X, labels)\nprint(f\"Silhouette score: {score}\")\n\nSilhouette score: 0.02340348182286992\nA reminder that the silhuette score reflects how defined the borders of clusters are, and even with taking a smaller portion of the data to spead of teasting and getting the highest score possible with a number of clusters around 20, the silhuette score was still far to low to be usable, not even approaching 0.5.\nSo I tried some more data manipulation and reasearched to find a better algorithm to use. I ended up settling on the random forest classifier approach, and modified the way my data was stored to optimize its use with the RandomForestClassifier. I did this by seperating each verb attribute into a separate column in a new data frame, with the number of columns corresponding with the maximum number of attributes. I also shuffled thr rows from its original alphabetized form to rid the bias that might result from that and took the a random 40 thousand entries as my computer ended up running out of memory somewhere along the way when I used more of the data. This should still provide more than sufficient training data.\n\n# Gathering just an even 40k of the ~41k rows to make dealing with them a bit easier\ndf = df.sample(frac = 1).head(40000)\n\n# Split attributes column into separate columns\ndf_attributes = df['attributes'].apply(lambda x: pd.Series(x.split(',')))\ndf_attributes.columns = ['attribute{}'.format(i) for i in range(1, len(df_attributes.columns) + 1)]\n\n# Merge verb column with split attributes columns\ndf = pd.concat([df['verb'], df_attributes], axis=1)\n\n# Fill NaN values with empty strings\ndf.fillna(value='', inplace=True)\n\n# Create parallel data frame with separate columns for each attribute\nparallel_df = pd.DataFrame()\nfor column in df_attributes.columns:\n    attribute_values = df_attributes[column].unique()\n    for value in attribute_values:\n        parallel_df[f'{column}_{value}'] = df['verb'].where(df[column] == value, '')\n\n# Extract the verb and attribute columns from the dataframe\nverbs = df['verb']\nattributes = df.iloc[:, 1:]\n\nThis resulted in the following data frame:\nverb  attribute1  attribute2  attribute3  attribute4  attribute5  attribute6  attribute7 305100 ПОШМЫГАЙТЕ  VERB    perf    intr plur   impr    excl 304563 ПОЧУДЯТСЯ   VERB    perf    intr plur   3per    futr    indc 297352 ПОСКРИПЫВАТЬ    INFN    impf    intr 277886 ПОДСУШИВАЕТЕСЬ  VERB    impf    intr plur   2per    pres    indc 67659  ДОСПЕЮТ VERB    perf    intr plur   3per    futr    indc … … … … … … … … … 82775  ЗАКАЛЫВАЕШЬСЯ   VERB    impf    intr sing   2per    pres    indc 148750 НАДМАЧИВАЮТСЯ   VERB    impf    intr plur   3per    pres    indc 90144  ЗАНАВЕШИВАЕТ    VERB    impf    tran sing   3per    pres    indc 279045 ПОДТОЧАТ    VERB    perf    tran plur   3per    futr    indc 380842 РЯВКНУЛО    VERB    perf    intr neut   sing    past    indc 40000 rows × 8 columns\nI then vectorized it from this form and prepared the forest classifier:\n\nmlb = MultiLabelBinarizer()\nattribute_features = mlb.fit_transform(attributes.values)\nattribute_columns = mlb.classes_\n\nX_train, X_test, y_train, y_test = train_test_split(verbs, attribute_features, test_size=0.2, random_state=42)\n\nvectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))\nX_train_vectorized = vectorizer.fit_transform(X_train)\nX_test_vectorized = vectorizer.transform(X_test)\n\nmodel = RandomForestClassifier(max_depth=None, random_state=0, n_jobs=-1, class_weight=\"balanced_subsample\")\nmodel.fit(X_train_vectorized, y_train)\n\nI quickly wanted to visualize the data again now that I had seperated the attributes. I used matplotlib to make the following bar graph showing the frequency on the most common attributes. It had some issued like plotting the “intr Erro,” but you can see that the INFN and VERB columns, if stacked would equal 40,000, which goes along with our data set size, this also applies to the ‘impf’ and ‘perf’ attributes, which in Russian corresponds to imperfective and perfective verbs, which a verb can be one or the other, not neither nor both:\n\nI also tried to put together a word cloud of the attributes, though my graphing method may be flawed. It required me to drop all the empty attribute columns, which I did for the earlier visualization. But it also had me concatenate all of the verb attributes so I had uniform columns. The visualization still helps in seeing the frequency which corresponds to the size of the attributes below, which shows the biggest group fits into either ‘VERB impf’ or ‘VERB perf’:\n\nWith a better understanding of out data and our model already initialized, the reasons for choosing the RandomForestClassifier method are more clear, but I can outline them here:\n\nNon-linearity: RandomForestClassifier is a non-linear model that can capture complex relationships between input features (verbs) and output classes (attributes). Verbs can have various patterns and combinations of attributes, and a non-linear model like RandomForestClassifier can capture these intricate patterns better than linear models.\nRobust to irrelevant features: RandomForestClassifier can handle a large number of input features (verbs) without requiring feature selection or dimensionality reduction techniques. It automatically selects relevant features and assigns them appropriate importance during the training process. This is advantageous when dealing with a potentially large vocabulary of verbs (which I would say we have).\nHandling categorical data: RandomForestClassifier naturally handles categorical data, making it suitable for classifying verbs based on categorical attributes like tense and person. It can learn the decision boundaries and relationships between different attribute values, allowing it to make accurate predictions.\n\nWith that, let’s train!"
  },
  {
    "objectID": "posts/perceptron-blog-post/PerceptronBlog.html",
    "href": "posts/perceptron-blog-post/PerceptronBlog.html",
    "title": "Finn’s Perceptron Algorithm Blog Post",
    "section": "",
    "text": "More experimentation!\nLet’s create some non-linearly seperable synthetic data sets, just so we can prove my algorithm can frind the next closest line of seperability.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(54321)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-0.5, -0.5), (0.5, 0.5)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn the above plot of clouds, the dots very clearly overlap, and finding a line to divide them completely won’t be possible. Instead, we can have the algorithm keep trying it’s best until the max_steps is reached:\n\np = perceptron.Perceptron()\np.fit(X, y, max_steps = 10000)\n\nI set the max_steps here to be a bit higher so the algorithm can have more time to do its best. Here is the same p instance of the class Perceptron, with the same instance variable w of weights:\n\np.w\n\narray([ 1.14190182,  1.59493071, -0.73332539])\n\n\nAnd putting everything together, we get a line that tries to seperate both dot blobs:\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can do the same chack of the score:\n\np.score(X, y)\n\n0.62\n\n\nSo it looks like the score did not get to \\(1\\), and in the above graph you can see that there are a few yellow dots on the purple side and vice versa. Because of the random nature of how the vector w is initialized, some instances of running the algorithm will be better and some not so much.\nAgain we can also visualize the progress the algorithm made with the history graph:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIt looks like that having max_steps be higher really doesn’t help much, so let’s see what happens again with the same data with less iterations:\n\np = perceptron.Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAnd the score:\n\np.score(X, y)\n\n0.46\n\n\nAnd the history graph:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSo actually, it looks like the accuracy at times reached \\(0.6\\), depending on the final iteration and randomness from the index i within the number of observations, we can get a worse score."
  },
  {
    "objectID": "posts/gebru-post2/gebru2.html",
    "href": "posts/gebru-post2/gebru2.html",
    "title": "A Discussion of AI, Eugenics, and Their Risky Futures.",
    "section": "",
    "text": "Temnit Gebru, a prominent AI researcher and advocate for ethical AI, gave a talk at the Middlebury College Conference about Eugenics and the Promise of Utopia through Artificial General Intelligence on April 24th, 2023. Dr. Gebru discussed the exploitation of labor in the development of AI, the historical context of eugenics and its potential resurgence through AI, and the centralization of power in AI development. Her talk serves as a wake-up call for those who have been swept up in the hype surrounding AI and its supposed benefits\n\n\n\n\nDr. Gebru addressing students at Middlebury College, during her talk “Eugenics and the Promise of Utopia through Artificial General Intelligence.”\n\n\n\n\nDr. Gebru began by addressing historical development and economic impact of AI, stating that it has the potential to be the greatest force for economic change in our lifetime. However, she cautioned that this potential for change comes with a significant cost. Gebru pointed out that the development of AI has relied heavily on the exploitation of labor, particularly of low-paid workers who annotate and label datasets for machine learning algorithms. These workers often have little job security and no benefits, despite the fact that their work is essential to the development of AI. Gebru argued that the AI industry must take responsibility for this exploitation and work to provide better working conditions and protections for these workers.\nImportantly, Dr. Gebru also discussed the historical context of eugenics and its potential resurgence through AI. She pointed out that the eugenics movement of the early 20th century was based on the idea of improving the genetic quality of the human population through selective breeding and sterilization. The movement was based on the belief that certain groups of people were genetically inferior and that their reproduction should be discouraged. Gebru argues that AI has the potential to resurrect these dangerous ideas by perpetuating biases in data and algorithms. She pointed out that the lack of diversity in the tech industry has already resulted in biased algorithms, and that this problem will only get worse unless steps are taken to address it.\nDr. Gebru’s talk also highlighted the issue of centralization of power in AI development. She pointed out that a few large companies, such as OpenAI and Microsoft, are dominating the AI industry and setting the agenda for its development. This centralization of power is a concern because it limits the diversity of perspectives and voices involved in AI development. Gebru argued that the industry must work to decentralize power and encourage a broader range of actors to participate in AI development\nDr. Gebru’s talk serves as a reminder that AI is not a neutral technology. It has the potential to be used for good or for harm, and it is up to us to ensure that it is used in a responsible and ethical manner. The inherently biased nature of specific datasets and those that create them needs to be diligently accounted for. We cannot simply rely on the promises of AI proponents or the hype surrounding the technology. We must critically examine its impact on society and work to mitigate any negative effects.\nCritics, on the other hand, argue that these claims are overblown and represent a kind of techno-utopianism that is not based in reality. They point out that while AI has the potential to do a lot of good, it is not a magic solution and comes with its own set of risks and challenges. Connecting to this point, many of these idea of conflating AI to a higher power go back all the way to the first days of eugenics, and that AI will prevail over all bad and malignant forces. Much of this can also be connected with the evolution of historical protestant work ethic into present field of tech companies as a whole.\nOne of the key concerns with the idea of AI as a kind of religion is the potential for blind faith and the abdication of responsibility. If people begin to see AI as a kind of all-knowing and all-powerful entity, they may become complacent and fail to question its decisions or actions. This could lead to a loss of agency and a dangerous concentration of power in the hands of a few large corporate entities with little regard to their effects on societal bias and diversity outside of its effect on their share price.\nContinuing, another one of the most important points that Dr. Gebru made was that AI is not a fully well-defined term. There are many different types of AI, each with its own potential benefits and risks. Some types of AI, such as narrow AI, are already being used in many applications, such as image recognition and natural language processing. However, other types of AI, such as the term “artificial general intelligence” (Sometimes called AGI), are still largely hypothetical. Gebru argued that we must be careful not to conflate different types of AI or overstate their capabilities. Many of these claims are proposed by those who have no experience in the field of artificial intelligence or large model machine learning, and they only makes these claims for the benefit of shareholders and speculators.\nGebru also highlighted the need for those who call themselves “effective altruists” to consider the risks associated with AI development. Effective altruism is a philosophy that advocates for doing the most good possible with one’s resources. However, Gebru argued that many effective altruists have ignored the risks associated with AI development in their pursuit of doing good. She pointed out that AI has the potential to cause significant harm, and that effective altruists must take these risks seriously and work to mitigate.\n\n\n\n\nThe logo of the movement known as “Effective Altruism.” A movement started in the late 2000s, which, quoted from their website, “using evidence and reason to figure out how to benefit others as much as possible, and taking action on that basis”\n\n\n\n\nIn conclusion, the discussion of Dr. Temnit Gebru’s talk highlights the potential benefits and risks associated with the development and implementation of artificial intelligence. While AI has the potential to revolutionize various industries and improve our quality of life, it also poses significant challenges that require attention from policymakers and regulators.\nOne of the biggest challenges that AI poses is the risk of perpetuating biases and inequalities. As Gebru noted, AI systems can reproduce existing societal biases and lead to discrimination against marginalized groups. Additionally, the rapid pace of technological advancement means that there is often a lack of regulation and oversight, which can lead to unintended consequences and negative outcomes.\nTo mitigate these risks and ensure that AI is developed and used in a responsible and ethical manner, it is crucial that policymakers and regulators take action. This can involve developing clear guidelines and standards for the development and deployment of AI systems, as well as establishing regulatory bodies to oversee the implementation of these systems.\nAnother important area of focus for regulation and legislation is the potential impact of AI on employment and the economy. As AI systems become more advanced, there is a risk that they will displace workers and lead to significant job losses. This requires policymakers to develop strategies to ensure that the benefits of AI are distributed equitably, and that workers are protected and supported through any economic transitions.\nOverall, the development of AI as explored by Dr. Gebru is a complex and multifaceted issue that requires careful consideration and action from policymakers and regulators. By developing clear guidelines and standards, and establishing regulatory bodies to oversee the implementation of AI systems, we can ensure that AI is developed and used in a responsible and ethical manner, and that its benefits are distributed equitably. Failure to take action risks perpetuating existing inequalities and exacerbating the challenges that we face as a society. It is therefore crucial that we prioritize the development of effective regulation and legislation that can guide the responsible and ethical use of AI, and ensure that its benefits are shared by all."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/penguins-blog-post/Penguins.html",
    "href": "posts/penguins-blog-post/Penguins.html",
    "title": "Finn’s Palmer Penguins Classification",
    "section": "",
    "text": "From left to right we have \\(Pygoscelis\\) \\(papua\\), AKA the Gentoo penguin, \\(Pygoscelis\\) \\(antarctica\\), AKA the appropriately named Chinstrap penguin, and finally \\(Pygoscelis\\) \\(adeliae\\), AKA the Adélie penguin.\nThe Palmer Penguins data set is a collection of physiological measurements for penguins collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, which is part of the Long Term Ecological Research Network. The data set contains measurements such as bill length, flipper length, and body mass for individuals from each of the three species of penguins: chinstrap, adelie, and gentoo. By analyzing these measurements, we can use various machine learning algorithms to classify each penguin into their respective species. This classification task can be performed by training a machine learning model on a portion of the data set and then using the trained model to predict the species of penguins in the remaining portion of the data set. By doing so, we can identify patterns and relationships in the data that can be used to accurately classify penguins into their respective species. We’ll decide later on which regression model to do the work. First lets import Pandas and the training data for use later.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nThe following code is from a machine learning pipeline where data preprocessing is performed using the scikit-learn library in Python.\nThe first line imports the LabelEncoder class which is used to encode categorical data. The next three lines create an instance of the LabelEncoder class, fits the encoder to the \\(Species\\) column of the training dataset, and creates a list of unique species names from the fitted encoder. The remaining code defines a function called \\(prepare\\)_\\(data\\) that drops certain columns, removes rows with missing or invalid data, encodes the \\(Species\\) column using the LabelEncoder’s transform() method, and performs one-hot encoding on the remaining features using the get_dummies() method. Finally, the function returns the preprocessed DataFrame and encoded \\(Species\\) values as separate arrays. The last two lines of code call the \\(prepare\\)_\\(data\\) function on the training dataset and outputs the preprocessed training columns.\n\nfrom sklearn.preprocessing import LabelEncoder # Import label encoder for tagging each data entry\n\nLabelEn = LabelEncoder() # creates an instance of the LabelEncoder class\nLabelEn.fit(train[\"Species\"]) # fits the LabelEncoder to the \"Species\" column of the train dataset using the fit() method\n\nspecies = [p.split()[0] for p in LabelEn.classes_] # creates a list called \"species\" that contains only the first word of each unique value in the \"Species\" column\n\ndef prepare_data(df): # defines a function called \"prepare_data\" that takes a DataFrame as input\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # drops several columns from the DataFrame using the drop() method\n    df = df[df[\"Sex\"] != \".\"] # drops any rows where the \"Sex\" column contains a period (\".\")\n    df = df.dropna() # drops any remaining rows that contain missing values\n    y = le.transform(df[\"Species\"]) # encodes the \"Species\" column using the LabelEncoder's transform() method\n    df = df.drop([\"Species\"], axis = 1) # one-hot encodes the remaining features of the DataFrame using the get_dummies() method\n    df = pd.get_dummies(df)\n    return df, y # returns the preprocessed DataFrame and the encoded \"Species\" values as separate arrays\n\nX_train, y_train = prepare_data(train)\nX_train # Output the preprocessed training columns\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nAnd above we’ve got a look at the dataframe we created, with all 14 columns of data points ranging from Culmen length to if they are male. Quickly though, let’s look at the context which classifies each of these three species into their respective groups.\n\n\n\n\nThe code below searches for the combination of categorical and numerical columns that achieve the highest accuracy score on the training data. The itertools library’s combinations function is used to generate all possible combinations of categorical and numerical columns from the all_qual_cols and all_quant_cols lists, respectively. The LogisticRegression class from scikit-learn is used to train a logistic regression model on each combination of columns, and the accuracy score of each model is compared to identify the combination that yields the highest score. The final result is printed out as a list of column names that produced the highest accuracy score on the training data. I decided to use logistic regression.\nLogistic regression is a good choice when the dependent variable is categorical and we want to predict the probability of a specific outcome. In this case, the dependent variable is categorical (i.e., the species of penguins), and we are trying to predict the probability of a penguin belonging to each of the three species.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom warnings import filterwarnings\nfilterwarnings('ignore') # Hiding the max iteration convergence warning\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", 'Island']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfinal_score = 0\nfinal_col = 0\n\nqual_idx = 0 # value used to index 'all_quant_cols' to get current value\n\nwhile qual_idx < len(all_qual_cols):\n    # loops through all combinations of categorical and numerical columns, \n    # and finally prints out the combination of columns that achieved the \n    # highest accuracy score on the training data.\n    qual = all_qual_cols[qual_idx]\n    qual_cols = [col for col in X_train.columns if qual in col ]\n    pair_idx = 0 # starts at 0 and is incremented by 1 until done\n    while pair_idx < len(all_quant_cols) - 1:\n        pair = (all_quant_cols[pair_idx], all_quant_cols[pair_idx + 1])\n        cols = list(pair) + qual_cols\n        LR = LogisticRegression()\n        LR.fit(X_train[cols], y_train)\n        if final_score < LR.score(X_train[cols], y_train):\n            final_col = cols\n            final_score = LR.score(X_train[cols], y_train)\n        pair_idx += 1\n    qual_idx += 1\n    \nprint(final_col)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Create a visualization\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\nplt.show()\n\n\n\n\nAbove you can see the graphs depicting three islands with penguins where we already know to which species each group belongs. Later we can compare how accurate our prediction is versus the actual data.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[final_col], y_train)\nLR.score(X_train[final_col], y_train)\n\n1.0\n\n\nEventually we end up with a score of 1.0, which is perfect!\nNext we can look how Culmen length and depth vary across species and across the islands.\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(min).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      34.5\n      13.1\n    \n    \n      Dream\n      32.1\n      15.5\n    \n    \n      Torgersen\n      33.5\n      16.6\n    \n  \n\n\n\n\n\ntrain[[\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby(\"Species\").mean().plot.bar(rot=20)\nplt.show(sns)\n\n\n\n\nWith info from the above graphs we can see the rough distrobutions across all three islands, and it looks like only Adelie penguins were on Torgersen island and the other two had Adelie and Gentoo or Adelie and Chinstraps.\n\nfrom matplotlib.patches import Patch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    # Get the first two columns of the input dataframe\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    \n    # Get the qualitative features of the input dataframe\n    qual_features = X.columns[2:]\n    \n    # Create a figure with subplots for each qualitative feature\n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # Create a grid to evaluate the model over\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    # Flatten the grid for easier evaluation\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      # Create a dataframe with all values set to 0\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      # Set the current qualitative feature to 1\n      for j in qual_features:\n        XY[j] = 0\n      XY[qual_features[i]] = 1\n\n      # Predict the class for each point in the grid and reshape the result\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      # Use a contour plot to visualize the decision regions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      # Get the data points that correspond to the current qualitative feature\n      ix = X[qual_features[i]] == 1\n      \n      # Plot the data points\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      # Set the axis labels\n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      # Create a legend for the different classes\n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      # Adjust the spacing between subplots\n      plt.tight_layout()\n\nThe above code defines a function named \\(plot\\)_\\(regions\\) that takes three arguments: model, \\(X\\), and \\(y\\). This function is intended to be used for plotting the decision boundaries of a classifier for a dataset with two continuous features and one or more categorical features.\nThe first few lines of the function extract the two continuous features (i.e., the first two columns) and the categorical features from the input data \\(X\\). Then, the function creates a grid of points spanning the range of the two continuous features, which is used for plotting the decision boundaries.\nThe main loop of the function iterates over each categorical feature in turn. For each feature, a new DataFrame \\(XY\\) is created that includes all combinations of the two continuous features and all possible values of the categorical feature (i.e., 0 or 1). The classifier model is then used to predict the class labels for each point in this grid, and the resulting predictions are plotted using a contour plot.\nWe will use this code to visualize decision boundaries of a classification model trained on the Palmer Penguins dataset, with different qualitative features.\n\n\n\n\nplt.close() # Clearing the old plot\nplot_regions(LR, X_train[final_col], y_train)\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nsns.relplot(\n    data=test,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\nplt.show(sns)\n\n\n\n\n\n\n\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[final_col], y_test)\n\n1.0\n\n\nAbove was a lot to take in, but in short we plot the decision boundary made by the \\(plot\\)_\\(regions\\) function and we can compare our decision with the actual labels from the original data. We got a score of \\(1.0\\) which means our model works on 100% of our penguin data points to predict which species they belong to based on culmen depth and length. We divided the penguins across three groups cutting two lines between them, labeling the Adelie red, Gentoo blue, and Chinstrap Green. Maybe now the next time you see one of these three species, you will be able to identify them!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning About Machine Learning, Linguistics, Brains, and so much more!",
    "section": "",
    "text": "Finn’s attempt at iOS development.\n\n\n\n\n\n\nApr 6, 2024\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s response and exploration of the ideas proposed by Arvind Narayanan surrounding fairness and bias.\n\n\n\n\n\n\nMay 19, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThis is Finn’s final project for the CS 451 Machine Learning class. It incorporates knowledge of the Russian langauge and much of what was taught in class to build an interactive language model to predict verb attributes for any given input Russian verb or infinitive.\n\n\n\n\n\n\nMay 15, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s experiment to delve into the realm of unsupervised learning to explore its potential in compressing image data. Through systematic experimentation with singular value decomposition (SVD), Finn aims to uncover the effectiveness of this technique in reducing the size of image files.\n\n\n\n\n\n\nMay 3, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nTW: This post contains thoughts and ideas of the author and Dr. Timnit Gebru surrounding topics such at eugenics. Finn’s exploration of the ways AI and unregulated bias mitigation might negatively impact the future of the field and amplify inequality. Notes from the talk given by Dr. Timnit Gebru.\n\n\n\n\n\n\nApr 28, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s quick look at Timnit Gebru’s views on the current situation of ethics in AI.\n\n\n\n\n\n\nApr 19, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s attempt to classify three species of penguins.\n\n\n\n\n\n\nMar 28, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s implementation of the perceptron algorithm.\n\n\n\n\n\n\nMar 14, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s first blog post for CSCI 0451.\n\n\n\n\n\n\nFeb 15, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Finn Ellingwood is a Computer Science Major at Middlebury College studying linguistics."
  }
]