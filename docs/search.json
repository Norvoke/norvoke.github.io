[
  {
    "objectID": "posts/post-2023-02-15/index.html",
    "href": "posts/post-2023-02-15/index.html",
    "title": "First Blog Post",
    "section": "",
    "text": "This is my first post to my official blog for CSCI 0451. I am testing out some things and will be working on implementing the perceptron algorithm in python to draw a line between two clouds of points."
  },
  {
    "objectID": "posts/post-2023-02-15/index.html#math",
    "href": "posts/post-2023-02-15/index.html#math",
    "title": "First Blog Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron-blog-post/PerceptronBlog.html",
    "href": "posts/perceptron-blog-post/PerceptronBlog.html",
    "title": "Finn’s Perceptron Algorithm Blog Post",
    "section": "",
    "text": "More experimentation!\nLet’s create some non-linearly seperable synthetic data sets, just so we can prove my algorithm can frind the next closest line of seperability.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(54321)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-0.5, -0.5), (0.5, 0.5)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn the above plot of clouds, the dots very clearly overlap, and finding a line to divide them completely won’t be possible. Instead, we can have the algorithm keep trying it’s best until the max_steps is reached:\n\np = perceptron.Perceptron()\np.fit(X, y, max_steps = 10000)\n\nI set the max_steps here to be a bit higher so the algorithm can have more time to do its best. Here is the same p instance of the class Perceptron, with the same instance variable w of weights:\n\np.w\n\narray([ 1.14190182,  1.59493071, -0.73332539])\n\n\nAnd putting everything together, we get a line that tries to seperate both dot blobs:\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nWe can do the same chack of the score:\n\np.score(X, y)\n\n0.62\n\n\nSo it looks like the score did not get to \\(1\\), and in the above graph you can see that there are a few yellow dots on the purple side and vice versa. Because of the random nature of how the vector w is initialized, some instances of running the algorithm will be better and some not so much.\nAgain we can also visualize the progress the algorithm made with the history graph:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nIt looks like that having max_steps be higher really doesn’t help much, so let’s see what happens again with the same data with less iterations:\n\np = perceptron.Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nAnd the score:\n\np.score(X, y)\n\n0.46\n\n\nAnd the history graph:\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nSo actually, it looks like the accuracy at times reached \\(0.6\\), depending on the final iteration and randomness from the index i within the number of observations, we can get a worse score."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning About Machine Learning",
    "section": "",
    "text": "Finn’s attempt to classify three species of penguins.\n\n\n\n\n\n\nMar 28, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s implementation of the perceptron algorithm.\n\n\n\n\n\n\nMar 14, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nFinn’s first blog post for CSCI 0451.\n\n\n\n\n\n\nFeb 15, 2023\n\n\nFinn Ellingwood\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/penguins-blog-post/Penguins.html",
    "href": "posts/penguins-blog-post/Penguins.html",
    "title": "Finn’s Palmer Penguins Classification",
    "section": "",
    "text": "From left to right we have \\(Pygoscelis\\) \\(papua\\), AKA the Gentoo penguin, \\(Pygoscelis\\) \\(antarctica\\), AKA the appropriately named Chinstrap penguin, and finally \\(Pygoscelis\\) \\(adeliae\\), AKA the Adélie penguin.\nThe Palmer Penguins data set is a collection of physiological measurements for penguins collected by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, which is part of the Long Term Ecological Research Network. The data set contains measurements such as bill length, flipper length, and body mass for individuals from each of the three species of penguins: chinstrap, adelie, and gentoo. By analyzing these measurements, we can use various machine learning algorithms to classify each penguin into their respective species. This classification task can be performed by training a machine learning model on a portion of the data set and then using the trained model to predict the species of penguins in the remaining portion of the data set. By doing so, we can identify patterns and relationships in the data that can be used to accurately classify penguins into their respective species. We’ll decide later on which regression model to do the work. First lets import Pandas and the training data for use later.\n\nimport pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\n\n\nThe following code is from a machine learning pipeline where data preprocessing is performed using the scikit-learn library in Python.\nThe first line imports the LabelEncoder class which is used to encode categorical data. The next three lines create an instance of the LabelEncoder class, fits the encoder to the \\(Species\\) column of the training dataset, and creates a list of unique species names from the fitted encoder. The remaining code defines a function called \\(prepare\\)_\\(data\\) that drops certain columns, removes rows with missing or invalid data, encodes the \\(Species\\) column using the LabelEncoder’s transform() method, and performs one-hot encoding on the remaining features using the get_dummies() method. Finally, the function returns the preprocessed DataFrame and encoded \\(Species\\) values as separate arrays. The last two lines of code call the \\(prepare\\)_\\(data\\) function on the training dataset and outputs the preprocessed training columns.\n\nfrom sklearn.preprocessing import LabelEncoder # Import label encoder for tagging each data entry\n\nLabelEn = LabelEncoder() # creates an instance of the LabelEncoder class\nLabelEn.fit(train[\"Species\"]) # fits the LabelEncoder to the \"Species\" column of the train dataset using the fit() method\n\nspecies = [p.split()[0] for p in LabelEn.classes_] # creates a list called \"species\" that contains only the first word of each unique value in the \"Species\" column\n\ndef prepare_data(df): # defines a function called \"prepare_data\" that takes a DataFrame as input\n    df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1) # drops several columns from the DataFrame using the drop() method\n    df = df[df[\"Sex\"] != \".\"] # drops any rows where the \"Sex\" column contains a period (\".\")\n    df = df.dropna() # drops any remaining rows that contain missing values\n    y = le.transform(df[\"Species\"]) # encodes the \"Species\" column using the LabelEncoder's transform() method\n    df = df.drop([\"Species\"], axis = 1) # one-hot encodes the remaining features of the DataFrame using the get_dummies() method\n    df = pd.get_dummies(df)\n    return df, y # returns the preprocessed DataFrame and the encoded \"Species\" values as separate arrays\n\nX_train, y_train = prepare_data(train)\nX_train # Output the preprocessed training columns\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nAnd above we’ve got a look at the dataframe we created, with all 14 columns of data points ranging from Culmen length to if they are male. Quickly though, let’s look at the context which classifies each of these three species into their respective groups.\n\n\n\n\nThe code below searches for the combination of categorical and numerical columns that achieve the highest accuracy score on the training data. The itertools library’s combinations function is used to generate all possible combinations of categorical and numerical columns from the all_qual_cols and all_quant_cols lists, respectively. The LogisticRegression class from scikit-learn is used to train a logistic regression model on each combination of columns, and the accuracy score of each model is compared to identify the combination that yields the highest score. The final result is printed out as a list of column names that produced the highest accuracy score on the training data. I decided to use logistic regression.\nLogistic regression is a good choice when the dependent variable is categorical and we want to predict the probability of a specific outcome. In this case, the dependent variable is categorical (i.e., the species of penguins), and we are trying to predict the probability of a penguin belonging to each of the three species.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom warnings import filterwarnings\nfilterwarnings('ignore') # Hiding the max iteration convergence warning\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", 'Island']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfinal_score = 0\nfinal_col = 0\n\nqual_idx = 0 # value used to index 'all_quant_cols' to get current value\n\nwhile qual_idx < len(all_qual_cols):\n    # loops through all combinations of categorical and numerical columns, \n    # and finally prints out the combination of columns that achieved the \n    # highest accuracy score on the training data.\n    qual = all_qual_cols[qual_idx]\n    qual_cols = [col for col in X_train.columns if qual in col ]\n    pair_idx = 0 # starts at 0 and is incremented by 1 until done\n    while pair_idx < len(all_quant_cols) - 1:\n        pair = (all_quant_cols[pair_idx], all_quant_cols[pair_idx + 1])\n        cols = list(pair) + qual_cols\n        LR = LogisticRegression()\n        LR.fit(X_train[cols], y_train)\n        if final_score < LR.score(X_train[cols], y_train):\n            final_col = cols\n            final_score = LR.score(X_train[cols], y_train)\n        pair_idx += 1\n    qual_idx += 1\n    \nprint(final_col)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Create a visualization\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\nplt.show()\n\n\n\n\nAbove you can see the graphs depicting three islands with penguins where we already know to which species each group belongs. Later we can compare how accurate our prediction is versus the actual data.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[final_col], y_train)\nLR.score(X_train[final_col], y_train)\n\n1.0\n\n\nEventually we end up with a score of 1.0, which is perfect!\nNext we can look how Culmen length and depth vary across species and across the islands.\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(min).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      34.5\n      13.1\n    \n    \n      Dream\n      32.1\n      15.5\n    \n    \n      Torgersen\n      33.5\n      16.6\n    \n  \n\n\n\n\n\ntrain[[\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby(\"Species\").mean().plot.bar(rot=20)\nplt.show(sns)\n\n\n\n\nWith info from the above graphs we can see the rough distrobutions across all three islands, and it looks like only Adelie penguins were on Torgersen island and the other two had Adelie and Gentoo or Adelie and Chinstraps.\n\nfrom matplotlib.patches import Patch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    # Get the first two columns of the input dataframe\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    \n    # Get the qualitative features of the input dataframe\n    qual_features = X.columns[2:]\n    \n    # Create a figure with subplots for each qualitative feature\n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # Create a grid to evaluate the model over\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    # Flatten the grid for easier evaluation\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      # Create a dataframe with all values set to 0\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      # Set the current qualitative feature to 1\n      for j in qual_features:\n        XY[j] = 0\n      XY[qual_features[i]] = 1\n\n      # Predict the class for each point in the grid and reshape the result\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      # Use a contour plot to visualize the decision regions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      # Get the data points that correspond to the current qualitative feature\n      ix = X[qual_features[i]] == 1\n      \n      # Plot the data points\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      # Set the axis labels\n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      # Create a legend for the different classes\n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      # Adjust the spacing between subplots\n      plt.tight_layout()\n\nThe above code defines a function named \\(plot\\)_\\(regions\\) that takes three arguments: model, \\(X\\), and \\(y\\). This function is intended to be used for plotting the decision boundaries of a classifier for a dataset with two continuous features and one or more categorical features.\nThe first few lines of the function extract the two continuous features (i.e., the first two columns) and the categorical features from the input data \\(X\\). Then, the function creates a grid of points spanning the range of the two continuous features, which is used for plotting the decision boundaries.\nThe main loop of the function iterates over each categorical feature in turn. For each feature, a new DataFrame \\(XY\\) is created that includes all combinations of the two continuous features and all possible values of the categorical feature (i.e., 0 or 1). The classifier model is then used to predict the class labels for each point in this grid, and the resulting predictions are plotted using a contour plot.\nWe will use this code to visualize decision boundaries of a classification model trained on the Palmer Penguins dataset, with different qualitative features.\n\n\n\n\nplt.close() # Clearing the old plot\nplot_regions(LR, X_train[final_col], y_train)\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nsns.relplot(\n    data=test,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\nplt.show(sns)\n\n\n\n\n\n\n\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[final_col], y_test)\n\n1.0\n\n\nAbove was a lot to take in, but in short we plot the decision boundary made by the \\(plot\\)_\\(regions\\) function and we can compare our decision with the actual labels from the original data. We got a score of \\(1.0\\) which means our model works on 100% of our penguin data points to predict which species they belong to based on culmen depth and length. We divided the penguins across three groups cutting two lines between them, labeling the Adelie red, Gentoo blue, and Chinstrap Green. Maybe now the next time you see one of these three species, you will be able to identify them!"
  },
  {
    "objectID": "posts/penguins-blog-post/Penguins.html#below-are-some-pictures-of-different-penguins-in-pygoscelis-genus-courtesy-of-wikimedia.-1",
    "href": "posts/penguins-blog-post/Penguins.html#below-are-some-pictures-of-different-penguins-in-pygoscelis-genus-courtesy-of-wikimedia.-1",
    "title": "Finn’s Palmer Penguins Classification",
    "section": "Below are some pictures of different penguins in Pygoscelis genus courtesy of wikimedia.",
    "text": "Below are some pictures of different penguins in Pygoscelis genus courtesy of wikimedia.\nThe code below searches for the combination of categorical and numerical columns that achieve the highest accuracy score on the training data. The itertools library’s combinations function is used to generate all possible combinations of categorical and numerical columns from the all_qual_cols and all_quant_cols lists, respectively. The LogisticRegression class from scikit-learn is used to train a logistic regression model on each combination of columns, and the accuracy score of each model is compared to identify the combination that yields the highest score. The final result is printed out as a list of column names that produced the highest accuracy score on the training data. I decided to use logistic regression.\nLogistic regression is a good choice when the dependent variable is categorical and we want to predict the probability of a specific outcome. In this case, the dependent variable is categorical (i.e., the species of penguins), and we are trying to predict the probability of a penguin belonging to each of the three species.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\nfrom warnings import filterwarnings\nfilterwarnings('ignore') # Hiding the max iteration convergence warning\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", 'Island']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\n\nfinal_score = 0\nfinal_col = 0\n\nqual_idx = 0 # value used to index 'all_quant_cols' to get current value\n\nwhile qual_idx < len(all_qual_cols):\n    # loops through all combinations of categorical and numerical columns, \n    # and finally prints out the combination of columns that achieved the \n    # highest accuracy score on the training data.\n    qual = all_qual_cols[qual_idx]\n    qual_cols = [col for col in X_train.columns if qual in col ]\n    pair_idx = 0 # starts at 0 and is incremented by 1 until done\n    while pair_idx < len(all_quant_cols) - 1:\n        pair = (all_quant_cols[pair_idx], all_quant_cols[pair_idx + 1])\n        cols = list(pair) + qual_cols\n        LR = LogisticRegression()\n        LR.fit(X_train[cols], y_train)\n        if final_score < LR.score(X_train[cols], y_train):\n            final_col = cols\n            final_score = LR.score(X_train[cols], y_train)\n        pair_idx += 1\n    qual_idx += 1\n    \nprint(final_col)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\n\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Create a visualization\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\nplt.show()\n\n\n\n\nAbove you can see the graphs depicting three islands with penguins where we already know to which species each group belongs. Later we can compare how accurate our prediction is versus the actual data.\n\nfrom sklearn.linear_model import LogisticRegression\n\n# this counts as 3 features because the two Clutch Completion \n# columns are transformations of a single original measurement. \n# you should find a way to automatically select some better columns\n# as suggested in the code block above\ncols = [\"Flipper Length (mm)\", \"Body Mass (g)\", \"Clutch Completion_No\", \"Clutch Completion_Yes\"]\n\nLR = LogisticRegression()\nLR.fit(X_train[final_col], y_train)\nLR.score(X_train[final_col], y_train)\n\n1.0\n\n\nEventually we end up with a score of 1.0, which is perfect!\nNext we can look how Culmen length and depth vary across species and across the islands.\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(min).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      34.5\n      13.1\n    \n    \n      Dream\n      32.1\n      15.5\n    \n    \n      Torgersen\n      33.5\n      16.6\n    \n  \n\n\n\n\n\ntrain[[\"Species\",\"Culmen Length (mm)\", \"Culmen Depth (mm)\"]].groupby(\"Species\").mean().plot.bar(rot=20)\nplt.show(sns)\n\n\n\n\nWith info from the above graphs we can see the rough distrobutions across all three islands, and it looks like only Adelie penguins were on Torgersen island and the other two had Adelie and Gentoo or Adelie and Chinstraps.\n\nfrom matplotlib.patches import Patch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    # Get the first two columns of the input dataframe\n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    \n    # Get the qualitative features of the input dataframe\n    qual_features = X.columns[2:]\n    \n    # Create a figure with subplots for each qualitative feature\n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # Create a grid to evaluate the model over\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    # Flatten the grid for easier evaluation\n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      # Create a dataframe with all values set to 0\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      # Set the current qualitative feature to 1\n      for j in qual_features:\n        XY[j] = 0\n      XY[qual_features[i]] = 1\n\n      # Predict the class for each point in the grid and reshape the result\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      # Use a contour plot to visualize the decision regions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      # Get the data points that correspond to the current qualitative feature\n      ix = X[qual_features[i]] == 1\n      \n      # Plot the data points\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      # Set the axis labels\n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      # Create a legend for the different classes\n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      # Adjust the spacing between subplots\n      plt.tight_layout()\n\nThe above code defines a function named \\(plot\\)_\\(regions\\) that takes three arguments: model, \\(X\\), and \\(y\\). This function is intended to be used for plotting the decision boundaries of a classifier for a dataset with two continuous features and one or more categorical features.\nThe first few lines of the function extract the two continuous features (i.e., the first two columns) and the categorical features from the input data \\(X\\). Then, the function creates a grid of points spanning the range of the two continuous features, which is used for plotting the decision boundaries.\nThe main loop of the function iterates over each categorical feature in turn. For each feature, a new DataFrame \\(XY\\) is created that includes all combinations of the two continuous features and all possible values of the categorical feature (i.e., 0 or 1). The classifier model is then used to predict the class labels for each point in this grid, and the resulting predictions are plotted using a contour plot.\nWe will use this code to visualize decision boundaries of a classification model trained on the Palmer Penguins dataset, with different qualitative features."
  },
  {
    "objectID": "posts/penguins-blog-post/Penguins.html#predict",
    "href": "posts/penguins-blog-post/Penguins.html#predict",
    "title": "Finn’s Palmer Penguins Classification",
    "section": "Predict",
    "text": "Predict\n\nplt.close() # Clearing the old plot\nplot_regions(LR, X_train[final_col], y_train)\n\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nsns.relplot(\n    data=test,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\nplt.show(sns)\n\n\n\n\n\n\n\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[final_col], y_test)\n\n1.0\n\n\nAbove was a lot to take in, but in short we plot the decision boundary made by the \\(plot\\)_\\(regions\\) function and we can compare our decision with the actual labels from the original data. We got a score of \\(1.0\\) which means our model works on 100% of our penguin data points to predict which species they belong to based on culmen depth and length. We divided the penguins across three groups cutting two lines between them, labeling the Adelie red, Gentoo blue, and Chinstrap Green. Maybe now the next time you see one of these three species, you will be able to identify them!"
  }
]