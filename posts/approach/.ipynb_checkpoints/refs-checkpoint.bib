@misc{narayanan2022limits,
  author       = {Narayanan, Arvind},
  howpublished = {Speech},
  title        = {The limits of the quantitative approach to discrimination},
  year         = {2022}
}

@article{landon2003physician,
    author = {Landon, Bruce E. and Normand, Sharon-Lise T. and Blumenthal, David and Daley, Jennifer},
    title = "{Physician Clinical Performance AssessmentProspects and Barriers}",
    journal = {JAMA},
    volume = {290},
    number = {9},
    pages = {1183-1189},
    year = {2003},
    month = {09},
    abstract = "{The performance of physicians in their day-to-day clinical practices
has become an area of intense public interest. Both patients and health care
purchasers want more effective means of identifying excellent clinicians,
and a variety of organizations are discussing and implementing plans for assessing
the performance of individual clinicians. In this article, we review the current
state of physician clinical performance assessment with a focus on its usefulness
for competency assessment. We describe recommendations for a physician clinical
performance assessment system for these purposes, and identify ways in which
current methods of performance assessment fall short of these. We conclude
that important technical barriers stand in the way of using physician clinical
performance assessment for evaluating the competency of individual physicians.
Overcoming these barriers will require considerable additional research and
development. Even then, for some uses, physician clinical performance assessment
at the individual physician level may be technically impossible to accomplish
in a valid and fair way.}",
    issn = {0098-7484},
    doi = {10.1001/jama.290.9.1183},
    url = {https://doi.org/10.1001/jama.290.9.1183},
    eprint = {https://jamanetwork.com/journals/jama/articlepdf/197217/jsc30188.pdf},
}

@article{pager2008sociology,
author = {Pager, Devah and Shepherd, Hana},
title = {The Sociology of Discrimination: Racial Discrimination in Employment, Housing, Credit, and Consumer Markets},
journal = {Annual Review of Sociology},
volume = {34},
number = {1},
pages = {181-209},
year = {2008},
doi = {10.1146/annurev.soc.33.040406.131740},
    note ={PMID: 20689680},

URL = { 
    
        https://doi.org/10.1146/annurev.soc.33.040406.131740
    
    

},
eprint = { 
    
        https://doi.org/10.1146/annurev.soc.33.040406.131740
}
}

@article{aylin2017semantics,
author = {Aylin Caliskan  and Joanna J. Bryson  and Arvind Narayanan },
title = {Semantics derived automatically from language corpora contain human-like biases},
journal = {Science},
volume = {356},
number = {6334},
pages = {183-186},
year = {2017},
doi = {10.1126/science.aal4230},
URL = {https://www.science.org/doi/abs/10.1126/science.aal4230},
eprint = {https://www.science.org/doi/pdf/10.1126/science.aal4230},
abstract = {AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.
}}

@article{crooks2012intro,
author = {Crooks, Andrew and Heppenstall, A.J.},
year = {2012},
month = {01},
pages = {85-105},
title = {Introduction to Agent-Based Modelling},
isbn = {978-90-481-8926-7},
journal = {Agent-Based Models of Geographical Systems},
doi = {10.1007/978-90-481-8927-4_5}
}

@article{hasan2020quantitative,
title = {Quantitative approaches for the evaluation of implementation research studies},
journal = {Psychiatry Research},
volume = {283},
pages = {112521},
year = {2020},
note = {VSI:Implementation Science},
issn = {0165-1781},
doi = {https://doi.org/10.1016/j.psychres.2019.112521},
url = {https://www.sciencedirect.com/science/article/pii/S0165178119307024},
author = {Justin D. Smith and Mohamed Hasan},
keywords = {Implementation measurement, Implementation research, Summative evaluation},
abstract = {Implementation research necessitates a shift from clinical trial methods in both the conduct of the study and in the way that it is evaluated given the focus on the impact of implementation strategies. That is, the methods or techniques to support the adoption and delivery of a clinical or preventive intervention, program, or policy. As strategies target one or more levels within the service delivery system, evaluating their impact needs to follow suit. This article discusses the methods and practices involved in quantitative evaluations of implementation research studies. We focus on evaluation methods that characterize and quantify the overall impacts of an implementation strategy on various outcomes. This article discusses available measurement methods for common quantitative implementation outcomes involved in such an evaluation—adoption, fidelity, implementation cost, reach, and sustainment—and the sources of such data for these metrics using established taxonomies and frameworks. Last, we present an example of a quantitative evaluation from an ongoing randomized rollout implementation trial of the Collaborative Care Model for depression management in a large primary healthcare system.}
}

@article{dressel2018accuracy,
author = {Julia Dressel  and Hany Farid },
title = {The accuracy, fairness, and limits of predicting recidivism},
journal = {Science Advances},
volume = {4},
number = {1},
pages = {eaao5580},
year = {2018},
doi = {10.1126/sciadv.aao5580},
URL = {https://www.science.org/doi/abs/10.1126/sciadv.aao5580},
eprint = {https://www.science.org/doi/pdf/10.1126/sciadv.aao5580},
abstract = {Should we trust computers to make life-altering decisions in the criminal justice system? Algorithms for predicting recidivism are commonly used to assess a criminal defendant’s likelihood of committing a crime. These predictions are used in pretrial, parole, and sentencing decisions. Proponents of these systems argue that big data and advanced machine learning make these analyses more accurate and less biased than humans. We show, however, that the widely used commercial risk assessment software COMPAS is no more accurate or fair than predictions made by people with little or no criminal justice expertise. In addition, despite COMPAS’s collection of 137 features, the same accuracy can be achieved with a simple linear classifier with only two features.}}

@article{lamont2002study,
	title = {The Study of Boundaries Across the Social Sciences},
	journal = {Annual Review of Sociology},
	volume = {28},
	year = {2002},
	pages = {167-95},
	abstract = {In recent years, the concept of boundaries has been at the center of influential research agendas in anthropology, history, political science, social psychology, and sociology. This article surveys some of these developments while describing the value added provided by the concept, particularly concerning the study of relational processes. It discusses literatures on (a) social and collective identity; (b) class, ethnic/racial, and gender/sex inequality; (c) professions, knowledge, and science; and (d) communities, national identities, and spatial boundaries. It points to similar processes at work across a range of institutions and social locations. It also suggests paths for further developments, focusing on the relationship between social and symbolic boundaries, cultural mechanisms for the production of boundaries, difference and hybridity, and cultural membership and group classifications.},
	author = {Lamont, Mich{\`e}le and Vir{\'a}g Moln{\'a}r}
}

@article{blodgett2020language,
    title = "Language (Technology) is Power: A Critical Survey of {``}Bias{''} in {NLP}",
    author = "Blodgett, Su Lin  and
      Barocas, Solon  and
      Daum{\'e} III, Hal  and
      Wallach, Hanna",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.485",
    doi = "10.18653/v1/2020.acl-main.485",
    pages = "5454--5476",
    abstract = "We survey 146 papers analyzing {``}bias{''} in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing {``}bias{''} is an inherently normative process. We further find that these papers{'} proposed quantitative techniques for measuring or mitigating {``}bias{''} are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing {``}bias{''} in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of {``}bias{''}---i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements{---}and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.",
}